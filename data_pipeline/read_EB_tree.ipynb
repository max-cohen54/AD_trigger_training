{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49b771ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source /cvmfs/sft.cern.ch/lcg/views/LCG_104/x86_64-centos7-gcc11-opt/setup.sh\n",
    "import numpy as np\n",
    "import ROOT\n",
    "import math\n",
    "import h5py\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7329f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019220\n"
     ]
    }
   ],
   "source": [
    "# Read the file and make the tree\n",
    "f = ROOT.TFile.Open(\"../temp/xaodanahelpersminimalexample/data/trees/user.mmcohen.data22_13p6TeV_AOD_EB_1-24-2024_tree.root/user.mmcohen.00440499.37026504._000001.tree.root\")\n",
    "#f = ROOT.TFile.Open(\"../temp/xaodanahelpersminimalexample/data/trees/EB_v1_1-20-2024_-1/data-tree/data22_13p6TeV.00440499.physics_EnhancedBias.merge.AOD.r15242_r15243_p6010_tid36850548_00.root\")\n",
    "\n",
    "td = f.Get(\"EB_Tree\")\n",
    "tree = td.Get(\"nominal\")\n",
    "nevents = tree.GetEntries() # number of events\n",
    "print(nevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9952d9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum numbers of each object\n",
    "NUM_JETS = 10\n",
    "NUM_ELECTRONS = 3\n",
    "NUM_MUONS = 3\n",
    "NUM_PHOTONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd2172cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min/max pt and eta values\n",
    "MIN_JET_PT = 0\n",
    "MIN_PHELMU_PT = 0\n",
    "MAX_ETA = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9485ed29-f399-41ab-9064-1ea058ae4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of triggers that we want to keep L1 events from for HLT training\n",
    "L1_save_trigs = [\n",
    "    'L1_3J35p0ETA23',\n",
    "    'L1_3J50',\n",
    "    'L1_4J15',\n",
    "    'L1_4J15p0ETA25',\n",
    "    'L1_4J20',\n",
    "    'L1_5J15p0ETA25',\n",
    "    'L1_J85_3J30',\n",
    "    'L1_HT150-J20s5pETA31_MJJ-400-CF',\n",
    "    'L1_HT190-J15s5pETA21',\n",
    "    'L1_SC111-CJ15',\n",
    "    'L1_DR-TAU20ITAU12I-J25',\n",
    "    'L1_TAU100',\n",
    "    'L1_TAU20IM_2TAU12IM_4J12p0ETA25',\n",
    "    'L1_TAU25IM_2TAU20IM_2J25_3J20',\n",
    "    'L1_TAU60_2TAU40',\n",
    "    'L1_TAU60_DR-TAU20ITAU12I',\n",
    "    'L1_2MU8F',\n",
    "    'L1_3MU3VF',\n",
    "    'L1_3MU5VF',\n",
    "    'L1_4MU3V',\n",
    "    'L1_MU10BOM',\n",
    "    'L1_MU12BOM',\n",
    "    'L1_MU14FCH',\n",
    "    'L1_MU5VF_3MU3VF',\n",
    "    'L1_MU8VF_2MU5VF',\n",
    "    'L1_BPH-0M9-EM7-EM5_2MU3V',\n",
    "    'L1_2EM8VH_MU8F',\n",
    "    'L1_2J15_XE55',\n",
    "    'L1_3J15p0ETA25_XE40',\n",
    "    'L1_EM15VHI_2TAU12IM_4J12',\n",
    "    'L1_EM15VHI_2TAU12IM_XE35',\n",
    "    'L1_EM15VH_MU8F',\n",
    "    'L1_MU8F_TAU12IM_3J12',\n",
    "    'L1_MU8F_TAU12IM_XE35',\n",
    "    'L1_MU8F_TAU20IM',\n",
    "    'L1_TAU40_2TAU12IM_XE40',\n",
    "    'L1_MJJ-500-NFF',\n",
    "    'L1_MU18VFCH',\n",
    "    'L1_J45p0ETA21_3J15p0ETA25',\n",
    "    'L1_MU8F_2J15_J20',\n",
    "    'L1_2EM20VH',\n",
    "    'L1_XE55',\n",
    "    'L1_EM18VHI_MJJ-300',\n",
    "    'L1_2EM15VHI',\n",
    "    'L1_J120',\n",
    "    'L1_EM20VH_3EM10VH',\n",
    "    'L1_MU5VF_EMPTY',\n",
    "    'L1_J40p0ETA25_2J25_J20p31ETA49',\n",
    "    'L1_2J50_XE40',\n",
    "    'L1_J25p0ETA23_2J15p31ETA49',\n",
    "    'L1_2MU5VF_3MU3V'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72a8c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_pad(in_data, max_objects):\n",
    "    \"\"\"input an object of size (4, num_objects), as well as the maximum number of objects wanted.\n",
    "    in_data[0] corresponds to the pt array, and the other three are E, eta, phi.\"\"\"\n",
    "    \n",
    "    indices = np.argsort(in_data[0])[-max_objects:][::-1] # indices that sort the in_data by pt\n",
    "    arrs = []\n",
    "    for array in in_data:\n",
    "\n",
    "        # Sorting\n",
    "        sorted_arr = array[indices]\n",
    "\n",
    "        # Padding\n",
    "        if len(array) < max_objects:\n",
    "            sorted_arr = np.concatenate((sorted_arr, np.zeros(max_objects - len(array))))\n",
    "\n",
    "        arrs.append(sorted_arr)\n",
    "        \n",
    "    \n",
    "    arrs = np.array(arrs).T\n",
    "    return np.array(arrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84df0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(min_pt, max_eta, pt, eta):\n",
    "    \"\"\"Make a mask on pt and eta of event, in case I want to make any data cuts.\"\"\"\n",
    "\n",
    "    return [(pt > MIN_JET_PT) and (abs(eta) < MAX_ETA) for pt, eta in zip(pt, eta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "294350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(events_lumiblocks, weights_file, run_file):\n",
    "    \"\"\"Collect the Enhanced Bias weights. Done by \n",
    "    reading the weights_file xml file which contains the weights for each event,\n",
    "    and the run_file xml file, which contains information regarding which\n",
    "    lumiblocks are bad.\n",
    "    \n",
    "    events_lumiblocks: list of tuples [('event_number', 'lumiblock_number'), (), ...], as strings.\"\"\"\n",
    "\n",
    "    # Parse the XML files\n",
    "    weights_tree = ET.parse(weights_file)\n",
    "    lumi_tree = ET.parse(run_file)\n",
    "\n",
    "    # Build the weights dictionary\n",
    "    weights_dict = {weight.get('id'): weight.get('value') for weight in weights_tree.findall('./weights/weight')}\n",
    "\n",
    "    # Build a dictionary for events to find their weights\n",
    "    event_weights = {event.get('n'): weights_dict.get(event.get('w')) for event in weights_tree.findall('./events/e')}\n",
    "\n",
    "    # Build a set of bad lumiblocks\n",
    "    bad_lumiblocks = {lb.get('id') for lb in lumi_tree.findall('./lb_list/lb[@flag=\"bad\"]')}\n",
    "\n",
    "    # Process each event-lumiblock pair\n",
    "    results = []\n",
    "    for event_number, lumiblock in events_lumiblocks:\n",
    "        event_weight = event_weights.get(event_number)\n",
    "        is_lumiblock_bad = lumiblock in bad_lumiblocks\n",
    "        results.append({\n",
    "            \"event_number\": event_number,\n",
    "            \"lumiblock\": lumiblock,\n",
    "            \"weight\": event_weight,\n",
    "            \"is_lumiblock_bad\": is_lumiblock_bad\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b47eb73d-d2ef-4751-b00e-a112235b5213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(event, c_name, vars=['pt', 'm', 'eta', 'phi'], mask=None):\n",
    "    \"\"\"\n",
    "    Extracts and processes data from a specified container within an event.\n",
    "\n",
    "    event: The event object containing the data.\n",
    "    c_name: Name of the container to get the data from.\n",
    "    vars: List of the variables to pull from the container.\n",
    "    mask: The mask to apply to the data, if applicable.\n",
    "    max_objects: Maximum number of objects to retain after sorting and padding.\n",
    "    return: Processed data from the specified container, after being sorted and padded.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for var in vars:\n",
    "        full_var_name = f\"{c_name}_{var}\"\n",
    "        var_data = getattr(event, full_var_name)\n",
    "\n",
    "        if mask is not None:\n",
    "            masked_data = np.asarray([x for x, m in zip(var_data, mask) if m])\n",
    "        else:\n",
    "            masked_data = np.asarray([x for x in var_data]) # for some reason this syntax runs way faster!?\n",
    "\n",
    "        data.append(masked_data)\n",
    "    return data\n",
    "    #return sort_and_pad(data, max_objects=max_objects)\n",
    "\n",
    "# Usage example:\n",
    "# jet_data = get_data(event, 'HLT_jet', vars=['pt', 'E', 'eta', 'phi'], mask=jet_mask, max_objects=NUM_JETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ebbfc94a-b2e6-4bae-b47f-02909324f2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1 / 1019220\n",
      "Progress: 10001 / 1019220\n",
      "Progress: 20001 / 1019220\n",
      "Progress: 30001 / 1019220\n",
      "Progress: 40001 / 1019220\n",
      "Progress: 50001 / 1019220\n",
      "Progress: 60001 / 1019220\n",
      "Progress: 70001 / 1019220\n",
      "Progress: 80001 / 1019220\n",
      "Progress: 90001 / 1019220\n",
      "Progress: 100001 / 1019220\n",
      "Progress: 110001 / 1019220\n",
      "Progress: 120001 / 1019220\n",
      "Progress: 130001 / 1019220\n",
      "Progress: 140001 / 1019220\n",
      "Progress: 150001 / 1019220\n",
      "Progress: 160001 / 1019220\n",
      "Progress: 170001 / 1019220\n",
      "Progress: 180001 / 1019220\n",
      "Progress: 190001 / 1019220\n",
      "Progress: 200001 / 1019220\n",
      "Progress: 210001 / 1019220\n",
      "Progress: 220001 / 1019220\n",
      "Progress: 230001 / 1019220\n",
      "Progress: 240001 / 1019220\n",
      "Progress: 250001 / 1019220\n",
      "Progress: 260001 / 1019220\n",
      "Progress: 270001 / 1019220\n",
      "Progress: 280001 / 1019220\n",
      "Progress: 290001 / 1019220\n",
      "Progress: 300001 / 1019220\n",
      "Progress: 310001 / 1019220\n",
      "Progress: 320001 / 1019220\n",
      "Progress: 330001 / 1019220\n",
      "Progress: 340001 / 1019220\n",
      "Progress: 350001 / 1019220\n",
      "Progress: 360001 / 1019220\n",
      "Progress: 370001 / 1019220\n",
      "Progress: 380001 / 1019220\n",
      "Progress: 390001 / 1019220\n",
      "Progress: 400001 / 1019220\n",
      "Progress: 410001 / 1019220\n",
      "Progress: 420001 / 1019220\n",
      "Progress: 430001 / 1019220\n",
      "Progress: 440001 / 1019220\n",
      "Progress: 450001 / 1019220\n",
      "Progress: 460001 / 1019220\n",
      "Progress: 470001 / 1019220\n",
      "Progress: 480001 / 1019220\n",
      "Progress: 490001 / 1019220\n",
      "Progress: 500001 / 1019220\n",
      "Progress: 510001 / 1019220\n",
      "Progress: 520001 / 1019220\n",
      "Progress: 530001 / 1019220\n",
      "Progress: 540001 / 1019220\n",
      "Progress: 550001 / 1019220\n",
      "Progress: 560001 / 1019220\n",
      "Progress: 570001 / 1019220\n",
      "Progress: 580001 / 1019220\n",
      "Progress: 590001 / 1019220\n",
      "Progress: 600001 / 1019220\n",
      "Progress: 610001 / 1019220\n",
      "Progress: 620001 / 1019220\n",
      "Progress: 630001 / 1019220\n",
      "Progress: 640001 / 1019220\n",
      "Progress: 650001 / 1019220\n",
      "Progress: 660001 / 1019220\n",
      "Progress: 670001 / 1019220\n",
      "Progress: 680001 / 1019220\n",
      "Progress: 690001 / 1019220\n",
      "Progress: 700001 / 1019220\n",
      "Progress: 710001 / 1019220\n",
      "Progress: 720001 / 1019220\n",
      "Progress: 730001 / 1019220\n",
      "Progress: 740001 / 1019220\n",
      "Progress: 750001 / 1019220\n",
      "Progress: 760001 / 1019220\n",
      "Progress: 770001 / 1019220\n",
      "Progress: 780001 / 1019220\n",
      "Progress: 790001 / 1019220\n",
      "Progress: 800001 / 1019220\n",
      "Progress: 810001 / 1019220\n",
      "Progress: 820001 / 1019220\n",
      "Progress: 830001 / 1019220\n",
      "Progress: 840001 / 1019220\n",
      "Progress: 850001 / 1019220\n",
      "Progress: 860001 / 1019220\n",
      "Progress: 870001 / 1019220\n",
      "Progress: 880001 / 1019220\n",
      "Progress: 890001 / 1019220\n",
      "Progress: 900001 / 1019220\n",
      "Progress: 910001 / 1019220\n",
      "Progress: 920001 / 1019220\n",
      "Progress: 930001 / 1019220\n",
      "Progress: 940001 / 1019220\n",
      "Progress: 950001 / 1019220\n",
      "Progress: 960001 / 1019220\n",
      "Progress: 970001 / 1019220\n",
      "Progress: 980001 / 1019220\n",
      "Progress: 990001 / 1019220\n",
      "Progress: 1000001 / 1019220\n",
      "Progress: 1010001 / 1019220\n"
     ]
    }
   ],
   "source": [
    "HLT_jet_list = []\n",
    "el_list = []\n",
    "LRT_el_list = []\n",
    "muon_list = []\n",
    "LRT_muon_list = []\n",
    "ph_list = []\n",
    "MET_list = []\n",
    "trig_list = []\n",
    "ev_lb_list = []\n",
    "pass_L1_unprescaled = []\n",
    "\n",
    "for i, event in enumerate(tree):\n",
    "    \n",
    "    # Collect event number and lumiblock for future weight calculation\n",
    "    ev_num = str(event.eventNumber)\n",
    "    lb_num = str(event.lumiBlock)\n",
    "    ev_lb_list.append((ev_num, lb_num))\n",
    "\n",
    "    # Masks\n",
    "    # HLT_jet_mask = make_mask(min_pt=MIN_JET_PT, max_eta=MAX_ETA, pt=event.HLT_jet_pt, eta=event.HLT_jet_eta)\n",
    "    # el_mask = make_mask(min_pt=MIN_PHELMU_PT, max_eta=MAX_ETA, pt=event.HLT_el_pt, eta=event.HLT_el_eta)\n",
    "    # el_LRT_mask = make_mask(min_pt=MIN_PHELMU_PT, max_eta=MAX_ETA, pt=event.HLT_el_LRT_pt, eta=event.HLT_el_LRT_eta)\n",
    "    # muon_mask = make_mask(min_pt=MIN_PHELMU_PT, max_eta=MAX_ETA, pt=event.HLT_muon_pt, eta=event.HLT_muon_eta)\n",
    "    # muon_LRT_mask = make_mask(min_pt=MIN_PHELMU_PT, max_eta=MAX_ETA, pt=event.HLT_muon_LRT_pt, eta=event.HLT_muon_LRT_eta)\n",
    "    # ph_mask = make_mask(min_pt=MIN_PHELMU_PT, max_eta=MAX_ETA, pt=event.ph_pt, eta=event.ph_eta)\n",
    "\n",
    "    # HLT Jets\n",
    "    HLT_jet_data = get_data(event, 'HLT_jet', vars=['pt', 'E', 'eta', 'phi'])\n",
    "    # HLT_jet_data = get_data(event, 'HLT_jet', vars=['pt', 'E', 'eta', 'phi'], mask=jet_mask)\n",
    "    HLT_jet_list.append(sort_and_pad(HLT_jet_data, max_objects=NUM_JETS))\n",
    "\n",
    "    # Electrons\n",
    "    el_data = get_data(event, 'HLT_el', vars=['pt', 'm', 'eta', 'phi'])\n",
    "    el_list.append(sort_and_pad(el_data, max_objects=NUM_ELECTRONS))\n",
    "\n",
    "    # LRT Electrons\n",
    "    LRT_el_data = get_data(event, 'HLT_el_LRT', vars=['pt', 'm', 'eta', 'phi'])\n",
    "    LRT_el_list.append(sort_and_pad(LRT_el_data, max_objects=NUM_ELECTRONS))\n",
    "\n",
    "    # Muons\n",
    "    muon_data = get_data(event, 'HLT_muon', vars=['pt', 'm', 'eta', 'phi'])\n",
    "    muon_list.append(sort_and_pad(muon_data, max_objects=NUM_MUONS))\n",
    "\n",
    "    # LRT Muons\n",
    "    LRT_muon_data = get_data(event, 'HLT_muon_LRT', vars=['pt', 'm', 'eta', 'phi'])\n",
    "    LRT_muon_list.append(sort_and_pad(LRT_muon_data, max_objects=NUM_MUONS))\n",
    "    \n",
    "    # Photons\n",
    "    ph_data = get_data(event, 'ph', vars=['pt', 'm', 'eta', 'phi'])\n",
    "    ph_list.append(sort_and_pad(ph_data, max_objects=NUM_PHOTONS))\n",
    "\n",
    "    # MET\n",
    "    MET_data = [np.float32(event.trigmetMet), 0, 0, np.float32(event.trigmetMetPhi)]\n",
    "    MET_list.append(MET_data)\n",
    "\n",
    "    # Trigger Decisions\n",
    "    trig_data = [str(trigger) for trigger in event.passedTriggers]\n",
    "    trig_list.append(trig_data)\n",
    "\n",
    "    # Check if the event passed any of the unprescaled physics triggers from the list:\n",
    "    pass_L1_unprescaled.append(1*any(trigger in L1_save_trigs for trigger in trig_data)) # 0=no 1=yes\n",
    "    \n",
    "\n",
    "    # Print progress\n",
    "    if (i % 10000) == 1:\n",
    "        print(f'Progress: {i} / {nevents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31f444b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = '/cvmfs/atlas.cern.ch/repo/sw/database/GroupData/TrigCostRootAnalysis/EnhancedBiasWeights_440499.xml'\n",
    "weights_run_file = '/cvmfs/atlas.cern.ch/repo/sw/database/GroupData/TrigCostRootAnalysis/enhanced_bias_run_440499.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b1eff685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect weights and lumiBlock information\n",
    "results = get_weights(events_lumiblocks=ev_lb_list, weights_file=weights_file, run_file=weights_run_file)\n",
    "weights_list = [np.float64(entry['weight']) for entry in results]\n",
    "lumiblock_bads_list = [entry['is_lumiblock_bad'] for entry in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "73957dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1309931188', '511'), ('1309979677', '511'), ('1309927610', '511'), ('1309926471', '511'), ('1309928291', '511')]\n",
      "[10.0, 479349.0, 12.0, 629.758, 453438.0, 105517.0, 137518.0, 39291.1, 453438.0, 137518.0]\n",
      "[False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "print(ev_lb_list[0:5])\n",
    "print(weights_list[0:10])\n",
    "print(lumiblock_bads_list[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5eb89e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019220\n",
      "[10.0, 479349.0, 12.0, 629.758, 453438.0]\n"
     ]
    }
   ],
   "source": [
    "print(len(weights_list))\n",
    "print(weights_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176813f7",
   "metadata": {},
   "source": [
    "# ___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4f2762a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../ntuples/AOD_EB_ntuples_1-26-2024.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"HLT_jets\", data=np.array(HLT_jet_list))\n",
    "    hf.create_dataset(\"electrons\", data=np.array(el_list))\n",
    "    hf.create_dataset(\"LRT_electrons\", data=np.array(LRT_el_list))\n",
    "    hf.create_dataset(\"muons\", data=np.array(muon_list))\n",
    "    hf.create_dataset(\"LRT_muons\", data=np.array(LRT_muon_list))\n",
    "    hf.create_dataset(\"photons\", data=np.array(ph_list))\n",
    "    hf.create_dataset(\"MET\", data=np.array(MET_list))\n",
    "    #hf.create_dataset(\"trig\", data=np.array(trig_list))\n",
    "    #hf.create_dataset(\"trigger_names\", data=trigger_names)\n",
    "    hf.create_dataset(\"pass_L1_unprescaled\", data=np.array(pass_L1_unprescaled))\n",
    "    hf.create_dataset(\"EB_weights\", data=weights_list)\n",
    "    hf.create_dataset(\"lumiblock_bads\", data=lumiblock_bads_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "360aa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "with h5py.File('../ntuples/AOD_EB_ntuples_1-26-2024.h5', 'r') as hf:\n",
    "    jets = hf['HLT_jets'][:]\n",
    "    electrons = hf['electrons'][:]\n",
    "    LRT_electrons = hf['LRT_electrons'][:]\n",
    "    muons = hf['muons'][:]\n",
    "    LRT_muons = hf['LRT_muons'][:]\n",
    "    photons = hf['photons'][:]\n",
    "    MET = hf['MET'][:].reshape(-1, 1, 4)  # Broadcasting MET\n",
    "    pass_L1_unprescaled = hf[\"pass_L1_unprescaled\"][:]\n",
    "    EB_weights = hf[\"EB_weights\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "90da6226-4f46-47dd-86a1-dd4118b382e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jets: (1019220, 10, 4)\n",
      "electrons: (1019220, 3, 4)\n",
      "LRT_electrons: (1019220, 3, 4)\n",
      "muons: (1019220, 3, 4)\n",
      "LRT_muons: (1019220, 3, 4)\n",
      "photons: (1019220, 3, 4)\n",
      "MET: (1019220, 1, 4)\n",
      "pass_L1_unprescaled: (1019220,)\n",
      "EB_weights: (1019220,)\n"
     ]
    }
   ],
   "source": [
    "print(f'jets: {jets.shape}')\n",
    "print(f'electrons: {electrons.shape}')\n",
    "print(f'LRT_electrons: {LRT_electrons.shape}')\n",
    "print(f'muons: {muons.shape}')\n",
    "print(f'LRT_muons: {LRT_muons.shape}')\n",
    "print(f'photons: {photons.shape}')\n",
    "print(f'MET: {MET.shape}')\n",
    "print(f'pass_L1_unprescaled: {pass_L1_unprescaled.shape}')\n",
    "print(f'EB_weights: {EB_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "562a0c87-4668-478c-a3cf-d469a9402e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejection: 609.5950386851063\n"
     ]
    }
   ],
   "source": [
    "print(f'rejection: {1 / (np.sum(EB_weights * pass_L1_unprescaled) / np.sum(EB_weights))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02b723-db21-475f-b069-1ffade0d0efd",
   "metadata": {},
   "source": [
    "## ____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b07707af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MET bias\n",
    "\n",
    "# MET: 0 --> 0.001 and -999 --> 0\n",
    "MET_zeros = data[:, 19, 0] == 0\n",
    "MET_999 = data[:, 19, 0] == -999\n",
    "data[MET_zeros, 19, 0] = 0.001\n",
    "data[MET_999, 19, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67446451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (1864687, 26, 4)\n",
      "trigger decisions: (1864687, 7)\n"
     ]
    }
   ],
   "source": [
    "non_zero_rows = np.any(data != 0, axis=(1, 2))\n",
    "data = data[non_zero_rows, :, :]\n",
    "trig = trig[non_zero_rows]\n",
    "print(f'data: {data.shape}')\n",
    "print(f'trigger decisions: {trig.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a435ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
