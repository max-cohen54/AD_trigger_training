{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c66c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20999b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "Dataset_short=Dataset[:,0:15,:]\n",
    "\n",
    "#for i, batch in enumerate(Dataset):\n",
    "#  pt_sum=0\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    if particle[3]!=0:\n",
    "#      pt_sum+=particle[0]\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    particle[0]=particle[0]/pt_sum\n",
    "    \n",
    "    \n",
    "Data_Train=Dataset_short[0:2000000,:,0:3]\n",
    "Data_Test=Dataset_short[2000001:3600000,:,0:3]\n",
    "Data_Validate=Dataset_short[3600001:4000000,:,0:3]\n",
    "\n",
    "\n",
    "\n",
    "Data_Train_Flat=np.reshape(Data_Train,(-1,45))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "591db785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def make_encoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(input_dim))\n",
    "    x=layers.BatchNormalization()(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z_mean=layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_logvar=layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_decoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    z=layers.Dense(input_dim)(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self,encoder,decoder,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta=1\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def set_beta(self,beta):\n",
    "        self.beta=beta\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            #making a masked loss function\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        \n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "        \n",
    "        reconstruction_loss*=(1-self.beta)\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        \n",
    "        #KL loss changed abck to sum as in paper\n",
    "        kl_loss = tf.reduce_sum(kl_loss)\n",
    "        \n",
    "        kl_loss *=self.beta\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f8c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 22s 1ms/step - loss: 11159987.0154 - reco_loss: 2233984.3178 - kl_loss: 1255665.6250 - val_loss: 1240.7700 - val_reco_loss: 14.4556 - val_kl_loss: 1226.3145 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 26.2663 - reco_loss: 22.5746 - kl_loss: 3.4165 - val_loss: 872.2685 - val_reco_loss: 11.2195 - val_kl_loss: 861.0490 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 20.9972 - reco_loss: 17.9928 - kl_loss: 2.9637 - val_loss: 833.8842 - val_reco_loss: 9.4429 - val_kl_loss: 824.4412 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 20.1711 - reco_loss: 17.1166 - kl_loss: 2.9180 - val_loss: 781.0735 - val_reco_loss: 21.8116 - val_kl_loss: 759.2620 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 18.1801 - reco_loss: 15.4191 - kl_loss: 2.7461 - val_loss: 778.8748 - val_reco_loss: 9.6605 - val_kl_loss: 769.2143 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 17.9440 - reco_loss: 15.1872 - kl_loss: 2.8580 - val_loss: 807.4637 - val_reco_loss: 8.7646 - val_kl_loss: 798.6991 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 17.4527 - reco_loss: 14.7154 - kl_loss: 2.7182 - val_loss: 763.4116 - val_reco_loss: 9.2502 - val_kl_loss: 754.1614 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 16.9390 - reco_loss: 14.2895 - kl_loss: 2.6752 - val_loss: 910.0566 - val_reco_loss: 7.4754 - val_kl_loss: 902.5812 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 31.3468 - reco_loss: 27.1513 - kl_loss: 13.0748 - val_loss: 906.0880 - val_reco_loss: 7.3269 - val_kl_loss: 898.7611 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 17.0332 - reco_loss: 13.9517 - kl_loss: 3.0295 - val_loss: 835.2823 - val_reco_loss: 8.4877 - val_kl_loss: 826.7946 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "17960/18000 [============================>.] - ETA: 0s - loss: 17.6984 - reco_loss: 13.6674 - kl_loss: 25.9674\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 17.7493 - reco_loss: 13.6685 - kl_loss: 25.9157 - val_loss: 787.7902 - val_reco_loss: 7.1879 - val_kl_loss: 780.6023 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 26.6675 - reco_loss: 23.8471 - kl_loss: 2.7933 - val_loss: 804.3889 - val_reco_loss: 7.2011 - val_kl_loss: 797.1877 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 15.0273 - reco_loss: 12.2944 - kl_loss: 2.7170 - val_loss: 786.3687 - val_reco_loss: 6.8993 - val_kl_loss: 779.4694 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 14.6756 - reco_loss: 11.9810 - kl_loss: 2.6978 - val_loss: 795.9671 - val_reco_loss: 6.8290 - val_kl_loss: 789.1381 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "17962/18000 [============================>.] - ETA: 0s - loss: 14.9244 - reco_loss: 12.2216 - kl_loss: 2.7016\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "18000/18000 [==============================] - 19s 1ms/step - loss: 14.9241 - reco_loss: 12.2213 - kl_loss: 2.7018 - val_loss: 792.5679 - val_reco_loss: 7.3179 - val_kl_loss: 785.2500 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#Here is a normalized model with (1-beta)rl beta*Kl loss\n",
    "beta=0.83\n",
    "vae_enc=make_encoder(45,32,16,3)\n",
    "vae_dec=make_decoder(45,32,16,3)\n",
    "vae_40MHZ=VAE_Model(vae_enc,vae_dec)\n",
    "vae_40MHZ.set_beta(beta)\n",
    "opt=keras.optimizers.Adam(learning_rate=0.001)\n",
    "vae_40MHZ.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "history = vae_40MHZ.fit(x=Data_Train_Flat, validation_split=0.1,epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks,shuffle=True)\n",
    "vae_40MHZ.save_weights(filepath='/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/Synthesis_testing/reduced_size_model/'.format(beta),save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a93fb8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None, 45)]                 0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 45)                   180       ['input_11[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_25 (Dense)            (None, 32)                   1472      ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 16)                   528       ['dense_25[0][0]']            \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 3)                    51        ['dense_26[0][0]']            \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 3)                    51        ['dense_26[0][0]']            \n",
      "                                                                                                  \n",
      " sampling_5 (Sampling)       (None, 3)                    0         ['z_mean[0][0]',              \n",
      "                                                                     'z_log_var[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2282 (8.91 KB)\n",
      "Trainable params: 2192 (8.56 KB)\n",
      "Non-trainable params: 90 (360.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 16)                64        \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 45)                1485      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2093 (8.18 KB)\n",
      "Trainable params: 2093 (8.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a=vae_40MHZ.predict(np.zeros((1,45)))\n",
    "vae_40MHZ.encoder.summary()\n",
    "vae_40MHZ.decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67750e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
