{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 20:53:44.999919: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 20:53:45.256629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AE(input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(z)\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(z)\n",
    "    outputs = layers.Dense(input_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)\n",
    "\n",
    "def mse_loss(true, prediction):\n",
    "    loss = np.mean(np.square(true - prediction), axis=-1)\n",
    "    return loss\n",
    "\n",
    "def AD_score(y, x):\n",
    "    # masked mse\n",
    "    mask = (y != 0)\n",
    "    _x = x * mask\n",
    "    _y = y * mask\n",
    "    return (mse_loss(_y, _x))\n",
    "\n",
    "\n",
    "class Model_Evaluator():\n",
    "  def __init__(self,model_path,backround,signal,title='placeholder',save=False,labels=None):\n",
    "    custom_objects = {'loss_fn': loss_fn}\n",
    "    self.model = load_model(model_path, custom_objects=custom_objects)\n",
    "    self.signal=signal\n",
    "    self.backround=backround\n",
    "    self.br_loss=[]\n",
    "    self.signal_loss=[]\n",
    "    self.backround_outputs=[]\n",
    "    self.signal_outputs=[]\n",
    "    self.title=title\n",
    "    self.saveplots=save\n",
    "    self.labels=labels\n",
    "\n",
    "  def calculate_loss(self,batch_size):\n",
    "\n",
    "    br=self.backround\n",
    "    self.backround_outputs=self.model.predict(br)\n",
    "    self.br_loss=AD_score(self.backround,self.backround_outputs)\n",
    "    for i, batch in enumerate(self.signal):\n",
    "      sr=batch\n",
    "      self.signal_outputs+=[self.model.predict(sr)]\n",
    "      self.signal_loss+=[AD_score(batch,self.signal_outputs[i])]\n",
    "    return [self.br_loss,self.signal_loss]\n",
    "\n",
    "\n",
    "  def histogram(self,bins):\n",
    "    plt.hist(self.br_loss,bins=bins,histtype='step',label='backround')\n",
    "    for i,batch in enumerate(self.signal_loss):\n",
    "      plt.hist(batch,bins=bins,histtype='step',label=self.labels[i])\n",
    "    plt.xlabel('loss')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"{}_Hist\".format(self.title))\n",
    "    plt.legend()\n",
    "    if self.saveplots==True:\n",
    "      plt.savefig(\"/Users/limsherm/Documents/Liam Rotation Coding Work/Data_Playaround/pt_normalization/{}_Hist.png\".format(self.title), format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "  def ROC(self):\n",
    "    plt.plot(np.linspace(0,1,1000),np.linspace(0,1,1000),'--',label='diagonal')\n",
    "    for j, batch in enumerate(self.signal_loss):\n",
    "      truth=[]\n",
    "      for i in range(len(self.br_loss)):\n",
    "        truth+=[0]\n",
    "      for i in range(len(batch)):\n",
    "        truth+=[1]\n",
    "      ROC_data=np.concatenate((self.br_loss,batch))\n",
    "      fpr,tpr,x=sk.roc_curve(truth,ROC_data)\n",
    "    #auc=np.trapz(tpr,fpr)\n",
    "      auc=sk.roc_auc_score(truth,ROC_data)\n",
    "      plt.plot(fpr,tpr,label=self.labels[j+1]+\": \"+str(auc))\n",
    "\n",
    "    plt.xlabel('fpr')\n",
    "    plt.semilogx()\n",
    "    plt.ylabel('trp')\n",
    "    plt.semilogy()\n",
    "    plt.title(\"{}_ROC\".format(self.title))\n",
    "    plt.legend()\n",
    "    if self.saveplots==True:\n",
    "      plt.savefig(\"/Users/limsherm/Documents/Liam Rotation Coding Work/Data_Playaround/pt_normalization/{}_ROC.png\".format(self.title), format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "  \n",
    "  def Find_AD_Cutoff(self,br_rate,desired_rate,starting_AD):\n",
    "    N=self.backround.shape[0]\n",
    "    AD_max=starting_AD\n",
    "    AD_List=np.linspace(0,AD_max,num=1000)\n",
    "    best_AD=0\n",
    "    for i,AD in enumerate(np.flip(AD_List)):\n",
    "      n=0\n",
    "      for loss in self.br_loss:\n",
    "        if loss>=AD:\n",
    "          n+=1\n",
    "      sigrate=br_rate*n/N\n",
    "      if sigrate<=desired_rate:\n",
    "        best_AD=AD\n",
    "      if sigrate>desired_rate:\n",
    "        break\n",
    "    self.AD_cutoff=best_AD\n",
    "    return best_AD\n",
    "    \n",
    "  def calculate_sensitivity(self,br_rate):\n",
    "    AD=self.AD_cutoff\n",
    "    sensitivity=[]\n",
    "    for i,losses in enumerate(self.signal_loss):\n",
    "        N=len(losses)\n",
    "        n=0\n",
    "        for loss in losses:\n",
    "            if loss>=AD:\n",
    "                n+=1\n",
    "        sen=n/N\n",
    "        sensitivity+=[sen]\n",
    "    self.signal_sensitivity=sensitivity\n",
    "    print(self.signal_sensitivity)\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]\n",
    "\n",
    "Data_Test_Flat=np.reshape(Data_Test,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one set done\n",
      "one set done\n",
      "one set done\n",
      "one set done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "h_to_Tau_Tau=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/hToTauTau_13TeV_PU20.h5','r')\n",
    "A_to_4_l=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/Ato4l_lepFilter_13TeV.h5','r')\n",
    "hC_to_Tau_Nu=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/hChToTauNu_13TeV_PU20.h5','r')\n",
    "lepto=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/leptoquark_LOWMASS_lepFilter_13TeV.h5','r')\n",
    "\n",
    "h_tt_set=np.array(h_to_Tau_Tau[\"Particles\"])\n",
    "hC_tn_set=np.array(hC_to_Tau_Nu[\"Particles\"])\n",
    "A_4l_set=np.array(A_to_4_l[\"Particles\"])\n",
    "lepto_set=np.array(lepto[\"Particles\"])\n",
    "sets=[h_tt_set,hC_tn_set,A_4l_set,lepto_set]\n",
    "#sets=[lepto_set]\n",
    "\n",
    "#sets2=[hC_tn_set,h_tt_set,A_4l_set,lepto_set]\n",
    "#signal_pt=[[],[],[],[]]\n",
    "#for i, signal in enumerate(sets2):\n",
    "#  for j in range(0,len(signal)):\n",
    "#    event_pT=np.sum(signal[j,:,0])\n",
    "#    signal_pt[i]+=[event_pT]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k, subset in enumerate(sets):\n",
    "  for i, batch in enumerate(subset):\n",
    "    pt_sum=0\n",
    "    for j, particle in enumerate(subset[i,:,:]):\n",
    "      if particle[3]!=0:\n",
    "        pt_sum+=particle[0]\n",
    "    for j, particle in enumerate(subset[i,:,:]):\n",
    "      particle[0]=particle[0]/pt_sum\n",
    "  print('one set done')\n",
    "\n",
    "normed_signals=[]\n",
    "for j, subset in enumerate(sets):\n",
    "    normed_signals+=[np.reshape(subset[:,:,0:3],(-1,57))]\n",
    "\n",
    "#pt_all=[]\n",
    "#pt_met=[]\n",
    "#multiplicity=[]\n",
    "#eta_average=[]\n",
    "#signals=sets\n",
    "#for i, signal in enumerate(signals):\n",
    "#  for j in range(0,len(signal)):\n",
    "#    multi=0\n",
    "#    event_pT=np.sum(signal[j,:,0])\n",
    "#    event_eta=np.average(signal[j,:,1])\n",
    "#    eta_average+=[event_eta]\n",
    "#    pt_all+=[event_pT]\n",
    "#    for k in range(0,19):\n",
    "#      if signal[j,k,3]==1:\n",
    "#        pt_met+=[signal[j,k,0]]\n",
    "#   #   if signal[j,k,3]!=0:\n",
    "  #      multi+=1\n",
    " #   multiplicity+=[multi]\n",
    "\n",
    "#hC_tn_data=np.reshape(sets[1][:,:,0:3],(-1,57))\n",
    "#h_tt_data=np.reshape(sets[0][:,:,0:3],(-1,57))\n",
    "#Test_Reshaped=np.reshape(Data_Test,(-1,57))\n",
    "#A_4l=np.reshape(sets[2][:,:,0:3],(-1,57))\n",
    "#lepto_data=np.reshape(sets[3][:,:,0:3],(-1,57))\n",
    "\n",
    "sig_label=['Backround','hC_tn','h_tt','A_4l','leptoquark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 72s 1ms/step\n",
      "21603/21603 [==============================] - 31s 1ms/step\n",
      "10048/23759 [===========>..................] - ETA: 19s"
     ]
    }
   ],
   "source": [
    "evaluation=Model_Evaluator('/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN.keras',Data_Test_Flat,normed_signals,title='Normalized 40MHZ DNN AE', save=False,labels=sig_label)\n",
    "evaluation.calculate_loss(1024)\n",
    "evaluation.histogram(bins=100)\n",
    "evaluation.ROC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.993399339933992"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.Find_AD_Cutoff(40*10**6,10,15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4465855517928258e-06, 6.576593640170886e-06, 5.360110060926584e-05, 0.0]\n"
     ]
    }
   ],
   "source": [
    "evaluation.calculate_sensitivity(40*10**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
