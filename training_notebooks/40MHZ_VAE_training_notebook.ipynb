{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f86c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 21:25:19.311061: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-18 21:25:19.498283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e950fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def make_encoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(input_dim))\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z_mean=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z_logvar=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "def make_encoder2(input_dim,h_dim_1,latent_dim):\n",
    "    inputs=keras.Input(shape=(input_dim))\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    z_mean=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z_logvar=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "def make_decoder2(input_dim,h_dim_1,latent_dim):\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    z=layers.Dense(input_dim, activation='relu')(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "def make_decoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    z=layers.Dense(input_dim, activation='relu')(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self,encoder,decoder,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta=1\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def set_beta(self,beta):\n",
    "        self.beta=beta\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            #making a masked loss function\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            kl_loss *= self.beta\n",
    "            #kl_loss *= 0\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        \n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        kl_loss *= self.beta\n",
    "        #kl_loss *= 0\n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc2afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71366461",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b6ddd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1800/1800 [==============================] - 10s 5ms/step - loss: 199.4568 - reco_loss: 184.2448 - kl_loss: 19.5830 - val_loss: 193.0461 - val_reco_loss: 170.4633 - val_kl_loss: 22.5828 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.7069 - reco_loss: 167.1680 - kl_loss: 22.6239 - val_loss: 195.8774 - val_reco_loss: 173.0973 - val_kl_loss: 22.7801 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.5211 - reco_loss: 166.6533 - kl_loss: 22.8847 - val_loss: 194.2883 - val_reco_loss: 171.2344 - val_kl_loss: 23.0539 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.5086 - reco_loss: 166.6320 - kl_loss: 22.9153 - val_loss: 197.3573 - val_reco_loss: 173.7827 - val_kl_loss: 23.5745 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1787/1800 [============================>.] - ETA: 0s - loss: 189.3673 - reco_loss: 166.4060 - kl_loss: 23.0178\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.3674 - reco_loss: 166.4058 - kl_loss: 23.0124 - val_loss: 193.9445 - val_reco_loss: 171.0085 - val_kl_loss: 22.9360 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.2860 - reco_loss: 166.2478 - kl_loss: 23.0476 - val_loss: 191.4013 - val_reco_loss: 168.1731 - val_kl_loss: 23.2282 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.2647 - reco_loss: 166.2336 - kl_loss: 23.0295 - val_loss: 195.6614 - val_reco_loss: 172.5939 - val_kl_loss: 23.0675 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.3536 - reco_loss: 166.2327 - kl_loss: 23.0984 - val_loss: 194.4975 - val_reco_loss: 171.0822 - val_kl_loss: 23.4154 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "1800/1800 [==============================] - 9s 5ms/step - loss: 189.2217 - reco_loss: 166.1431 - kl_loss: 23.0552 - val_loss: 194.5054 - val_reco_loss: 171.3411 - val_kl_loss: 23.1642 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "1800/1800 [==============================] - ETA: 0s - loss: 189.1022 - reco_loss: 166.0609 - kl_loss: 23.0161\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.1023 - reco_loss: 166.0609 - kl_loss: 23.0161 - val_loss: 193.2410 - val_reco_loss: 170.0867 - val_kl_loss: 23.1544 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.2211 - reco_loss: 166.1227 - kl_loss: 23.0987 - val_loss: 193.3804 - val_reco_loss: 170.2914 - val_kl_loss: 23.0890 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.2226 - reco_loss: 166.1739 - kl_loss: 23.0422 - val_loss: 193.2380 - val_reco_loss: 170.0789 - val_kl_loss: 23.1591 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.2600 - reco_loss: 166.2097 - kl_loss: 23.0627 - val_loss: 197.6454 - val_reco_loss: 174.5067 - val_kl_loss: 23.1387 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "1797/1800 [============================>.] - ETA: 0s - loss: 189.0934 - reco_loss: 166.0279 - kl_loss: 23.0428\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1800/1800 [==============================] - 8s 4ms/step - loss: 189.0936 - reco_loss: 166.0281 - kl_loss: 23.0420 - val_loss: 195.4842 - val_reco_loss: 172.3605 - val_kl_loss: 23.1237 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "vae_enc=make_encoder(57,32,16,3)\n",
    "vae_dec=make_decoder(57,32,16,3)\n",
    "vae_40MHZ=VAE_Model(vae_enc,vae_dec)\n",
    "vae_40MHZ.set_beta(10)\n",
    "vae_40MHZ.compile(optimizer='adam')\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "history = vae_40MHZ.fit(x=Data_Train_Flat,validation_split=0.1, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a14ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_40MHZ.save_weights(filepath='/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN_VAE_57_32_16_3_beta_10/',save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301a20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
