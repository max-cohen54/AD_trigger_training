{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c66c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 23:39:41.912523: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 23:39:42.024327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20999b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "#for i, batch in enumerate(Dataset):\n",
    "#  pt_sum=0\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    if particle[3]!=0:\n",
    "#      pt_sum+=particle[0]\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    particle[0]=particle[0]/pt_sum\n",
    "    \n",
    "    \n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]\n",
    "\n",
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591db785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def make_encoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(input_dim))\n",
    "    #x=layers.BatchNormalization()(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z_mean=layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_logvar=layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_decoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    z=layers.Dense(input_dim)(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self,encoder,decoder,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta=1\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def set_beta(self,beta):\n",
    "        self.beta=beta\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            #making a masked loss function\n",
    "            mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "            reconstruction_loss *=(1-self.beta)\n",
    "\n",
    "            kl_loss = tf.square(z_mean)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *=self.beta\n",
    "\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reco_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        z_mean, z_log_var, z = self.encoder(data)\n",
    "        \n",
    "        reconstruction = self.decoder(z)\n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.mse(mask*data, mask*reconstruction)))\n",
    "        \n",
    "        reconstruction_loss*=(1-self.beta)\n",
    "\n",
    "        kl_loss = tf.square(z_mean)\n",
    "        \n",
    "        #KL loss changed abck to sum as in paper\n",
    "        kl_loss = tf.reduce_sum(kl_loss)\n",
    "        \n",
    "        kl_loss *=self.beta\n",
    "        \n",
    "        total_loss = reconstruction_loss + kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reco_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "\n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f8c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 47s 2ms/step - loss: 850132284167.8502 - reco_loss: 850132284167.8228 - kl_loss: 97.0635 - val_loss: 28763.4238 - val_reco_loss: 403.0172 - val_kl_loss: 28360.4062 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 288.1376 - reco_loss: 225.7661 - kl_loss: 31.0757 - val_loss: 1190.3136 - val_reco_loss: 21.4795 - val_kl_loss: 1168.8341 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 24.0676 - reco_loss: 21.0410 - kl_loss: 2.4523 - val_loss: 435.5163 - val_reco_loss: 9.8400 - val_kl_loss: 425.6763 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 19.7117 - reco_loss: 18.0687 - kl_loss: 1.4806 - val_loss: 450.3217 - val_reco_loss: 10.7039 - val_kl_loss: 439.6178 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 18.0096 - reco_loss: 16.6868 - kl_loss: 1.1289 - val_loss: 182.3170 - val_reco_loss: 12.9919 - val_kl_loss: 169.3251 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 14.4748 - reco_loss: 13.7672 - kl_loss: 0.6520 - val_loss: 118.3680 - val_reco_loss: 6.6923 - val_kl_loss: 111.6757 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 47s 3ms/step - loss: 13.5383 - reco_loss: 13.0252 - kl_loss: 0.4825 - val_loss: 97.5208 - val_reco_loss: 6.6871 - val_kl_loss: 90.8337 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 12.9420 - reco_loss: 12.5318 - kl_loss: 0.3923 - val_loss: 93.0495 - val_reco_loss: 11.0739 - val_kl_loss: 81.9756 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 12.8363 - reco_loss: 12.4813 - kl_loss: 0.3550 - val_loss: 79.5520 - val_reco_loss: 5.5141 - val_kl_loss: 74.0379 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 48s 3ms/step - loss: 12.2321 - reco_loss: 11.8953 - kl_loss: 0.3321 - val_loss: 74.6984 - val_reco_loss: 9.0705 - val_kl_loss: 65.6279 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 12.3037 - reco_loss: 11.9954 - kl_loss: 0.2992 - val_loss: 68.0658 - val_reco_loss: 5.5757 - val_kl_loss: 62.4901 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 12.0660 - reco_loss: 11.7828 - kl_loss: 0.2780 - val_loss: 68.2932 - val_reco_loss: 5.3742 - val_kl_loss: 62.9190 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 11.4862 - reco_loss: 11.2262 - kl_loss: 0.2553 - val_loss: 65.2041 - val_reco_loss: 7.0411 - val_kl_loss: 58.1630 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 11.2551 - reco_loss: 11.0167 - kl_loss: 0.2340 - val_loss: 59.5302 - val_reco_loss: 6.3664 - val_kl_loss: 53.1638 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 11.1268 - reco_loss: 10.9060 - kl_loss: 0.2198 - val_loss: 60.0429 - val_reco_loss: 5.7674 - val_kl_loss: 54.2755 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 11.2208 - reco_loss: 11.0093 - kl_loss: 0.2084 - val_loss: 56.5097 - val_reco_loss: 5.9519 - val_kl_loss: 50.5578 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 11.4844 - reco_loss: 11.2779 - kl_loss: 0.1985 - val_loss: 51.4874 - val_reco_loss: 6.6113 - val_kl_loss: 44.8762 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.8146 - reco_loss: 10.6290 - kl_loss: 0.1862 - val_loss: 51.5517 - val_reco_loss: 11.0143 - val_kl_loss: 40.5374 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 11.0012 - reco_loss: 10.8150 - kl_loss: 0.1823 - val_loss: 46.6520 - val_reco_loss: 5.5935 - val_kl_loss: 41.0585 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 10.6047 - reco_loss: 10.4280 - kl_loss: 0.1771 - val_loss: 50.8399 - val_reco_loss: 5.9630 - val_kl_loss: 44.8769 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.3218 - reco_loss: 10.1518 - kl_loss: 0.1635 - val_loss: 43.3013 - val_reco_loss: 5.2037 - val_kl_loss: 38.0975 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 47s 3ms/step - loss: 10.7666 - reco_loss: 10.6125 - kl_loss: 0.1546 - val_loss: 46.9096 - val_reco_loss: 7.7239 - val_kl_loss: 39.1857 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 10.2187 - reco_loss: 10.0688 - kl_loss: 0.1482 - val_loss: 43.2749 - val_reco_loss: 5.8474 - val_kl_loss: 37.4275 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 10.4092 - reco_loss: 10.2618 - kl_loss: 0.1501 - val_loss: 44.3711 - val_reco_loss: 6.3625 - val_kl_loss: 38.0087 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 10.6993 - reco_loss: 10.5492 - kl_loss: 0.1530 - val_loss: 46.0780 - val_reco_loss: 6.4622 - val_kl_loss: 39.6158 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "18000/18000 [==============================] - 47s 3ms/step - loss: 10.3170 - reco_loss: 10.1690 - kl_loss: 0.1473 - val_loss: 42.5633 - val_reco_loss: 6.5672 - val_kl_loss: 35.9961 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 47s 3ms/step - loss: 10.2680 - reco_loss: 10.1199 - kl_loss: 0.1477 - val_loss: 44.6939 - val_reco_loss: 6.2413 - val_kl_loss: 38.4525 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.0693 - reco_loss: 9.9234 - kl_loss: 0.1418 - val_loss: 43.2920 - val_reco_loss: 6.4000 - val_kl_loss: 36.8920 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 9.9190 - reco_loss: 9.7838 - kl_loss: 0.1331 - val_loss: 44.7810 - val_reco_loss: 8.0437 - val_kl_loss: 36.7373 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 10.1123 - reco_loss: 9.9832 - kl_loss: 0.1264 - val_loss: 38.9870 - val_reco_loss: 6.2244 - val_kl_loss: 32.7626 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.2716 - reco_loss: 10.1434 - kl_loss: 0.1259 - val_loss: 39.1412 - val_reco_loss: 6.5560 - val_kl_loss: 32.5853 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.4624 - reco_loss: 10.3427 - kl_loss: 0.1176 - val_loss: 35.8488 - val_reco_loss: 6.9146 - val_kl_loss: 28.9341 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 9.6653 - reco_loss: 9.5522 - kl_loss: 0.1150 - val_loss: 38.8409 - val_reco_loss: 6.5931 - val_kl_loss: 32.2478 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 10.0484 - reco_loss: 9.9319 - kl_loss: 0.1135 - val_loss: 34.4435 - val_reco_loss: 6.1207 - val_kl_loss: 28.3228 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.1346 - reco_loss: 10.0232 - kl_loss: 0.1119 - val_loss: 36.0696 - val_reco_loss: 6.3678 - val_kl_loss: 29.7018 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 10.2215 - reco_loss: 10.1111 - kl_loss: 0.1126 - val_loss: 37.7581 - val_reco_loss: 8.9356 - val_kl_loss: 28.8225 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 9.8626 - reco_loss: 9.7534 - kl_loss: 0.1088 - val_loss: 36.6746 - val_reco_loss: 6.8423 - val_kl_loss: 29.8323 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "17988/18000 [============================>.] - ETA: 0s - loss: 9.7638 - reco_loss: 9.6574 - kl_loss: 0.1062\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 9.7640 - reco_loss: 9.6575 - kl_loss: 0.1062 - val_loss: 35.0879 - val_reco_loss: 8.4394 - val_kl_loss: 26.6485 - lr: 0.0010\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 45s 2ms/step - loss: 9.1013 - reco_loss: 9.0009 - kl_loss: 0.0995 - val_loss: 35.3801 - val_reco_loss: 8.6437 - val_kl_loss: 26.7364 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.7374 - reco_loss: 8.6404 - kl_loss: 0.0973 - val_loss: 34.1517 - val_reco_loss: 7.6725 - val_kl_loss: 26.4793 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.9326 - reco_loss: 8.8369 - kl_loss: 0.0947 - val_loss: 33.1318 - val_reco_loss: 7.4063 - val_kl_loss: 25.7255 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "18000/18000 [==============================] - 43s 2ms/step - loss: 8.7961 - reco_loss: 8.7031 - kl_loss: 0.0928 - val_loss: 32.0275 - val_reco_loss: 8.0955 - val_kl_loss: 23.9320 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.7911 - reco_loss: 8.6997 - kl_loss: 0.0909 - val_loss: 31.9631 - val_reco_loss: 8.0309 - val_kl_loss: 23.9322 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.6860 - reco_loss: 8.5962 - kl_loss: 0.0891 - val_loss: 30.7989 - val_reco_loss: 7.5310 - val_kl_loss: 23.2679 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.6097 - reco_loss: 8.5225 - kl_loss: 0.0868 - val_loss: 30.2141 - val_reco_loss: 7.5365 - val_kl_loss: 22.6776 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 8.5962 - reco_loss: 8.5100 - kl_loss: 0.0858 - val_loss: 30.1648 - val_reco_loss: 7.8851 - val_kl_loss: 22.2797 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 8.6410 - reco_loss: 8.5562 - kl_loss: 0.0846 - val_loss: 31.0024 - val_reco_loss: 8.2476 - val_kl_loss: 22.7549 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.8423 - reco_loss: 8.7578 - kl_loss: 0.0835 - val_loss: 29.4104 - val_reco_loss: 7.5481 - val_kl_loss: 21.8623 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "18000/18000 [==============================] - 43s 2ms/step - loss: 8.5454 - reco_loss: 8.4633 - kl_loss: 0.0823 - val_loss: 29.2742 - val_reco_loss: 7.7441 - val_kl_loss: 21.5301 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.6330 - reco_loss: 8.5519 - kl_loss: 0.0807 - val_loss: 26.5808 - val_reco_loss: 6.1176 - val_kl_loss: 20.4632 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.5232 - reco_loss: 8.4447 - kl_loss: 0.0788 - val_loss: 27.1672 - val_reco_loss: 7.1115 - val_kl_loss: 20.0557 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.5053 - reco_loss: 8.4271 - kl_loss: 0.0782 - val_loss: 28.7666 - val_reco_loss: 8.3942 - val_kl_loss: 20.3724 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.7075 - reco_loss: 8.6300 - kl_loss: 0.0776 - val_loss: 28.9813 - val_reco_loss: 7.3881 - val_kl_loss: 21.5933 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "17982/18000 [============================>.] - ETA: 0s - loss: 8.7816 - reco_loss: 8.7040 - kl_loss: 0.0774\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.7816 - reco_loss: 8.7039 - kl_loss: 0.0774 - val_loss: 29.2923 - val_reco_loss: 8.4956 - val_kl_loss: 20.7967 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "18000/18000 [==============================] - 44s 2ms/step - loss: 8.5315 - reco_loss: 8.4555 - kl_loss: 0.0763 - val_loss: 29.0565 - val_reco_loss: 8.2836 - val_kl_loss: 20.7729 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "18000/18000 [==============================] - 46s 3ms/step - loss: 8.7472 - reco_loss: 8.6709 - kl_loss: 0.0763 - val_loss: 29.0297 - val_reco_loss: 8.3057 - val_kl_loss: 20.7240 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "18000/18000 [==============================] - 45s 3ms/step - loss: 8.6826 - reco_loss: 8.6067 - kl_loss: 0.0761 - val_loss: 29.0321 - val_reco_loss: 8.3445 - val_kl_loss: 20.6876 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "17986/18000 [============================>.] - ETA: 0s - loss: 8.6783 - reco_loss: 8.6020 - kl_loss: 0.0760\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "18000/18000 [==============================] - 45s 2ms/step - loss: 8.6782 - reco_loss: 8.6019 - kl_loss: 0.0760 - val_loss: 28.9012 - val_reco_loss: 8.2108 - val_kl_loss: 20.6904 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "#Here is a normalized model with (1-beta)rl beta*Kl loss\n",
    "beta=0.83\n",
    "vae_enc=make_encoder(57,32,16,3)\n",
    "vae_dec=make_decoder(57,32,16,3)\n",
    "vae_40MHZ=VAE_Model(vae_enc,vae_dec)\n",
    "vae_40MHZ.set_beta(beta)\n",
    "opt=keras.optimizers.Adam(learning_rate=0.001)\n",
    "vae_40MHZ.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "history = vae_40MHZ.fit(x=Data_Train_Flat, validation_split=0.1,epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks,shuffle=True)\n",
    "vae_40MHZ.save_weights(filepath='/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/Different_VAE_Models/non_normed_new_beta_{}_v4_no_batchnorm_clipped_Kl/'.format(beta),save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93fb8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[ 1.00773662e-01,  2.35963464e-02, -4.27330405e-01],\n",
      "       [-2.83860475e-01,  1.34456813e-01,  4.44695801e-01],\n",
      "       [-1.10274255e-01,  1.48923874e-01,  1.87409967e-01],\n",
      "       [-4.33977574e-01, -1.59469247e-02,  2.61006743e-01],\n",
      "       [ 7.28425980e-02,  8.53597224e-02,  1.83448941e-01],\n",
      "       [-1.33358508e-01, -2.18769133e-01,  1.24764740e-02],\n",
      "       [ 4.93590534e-02, -1.99837923e-01, -3.06572318e-02],\n",
      "       [ 1.68306336e-01,  1.54072315e-01, -5.14794588e-01],\n",
      "       [ 2.08547428e-01,  3.90993387e-01, -2.09066123e-01],\n",
      "       [ 5.74311018e-02, -3.43665123e-01, -3.16123396e-01],\n",
      "       [-2.11883813e-01, -2.01519072e-01,  3.05890143e-02],\n",
      "       [-9.31852758e-02, -2.25590944e-01,  2.24801004e-02],\n",
      "       [ 1.28868312e-01,  4.07469153e-01, -2.99613476e-02],\n",
      "       [-2.23605633e-02,  3.43588471e-01, -2.26695225e-01],\n",
      "       [-2.03532130e-01,  3.14663380e-01,  4.36624289e-02],\n",
      "       [ 4.57111001e-01,  2.85661668e-01, -4.81497675e-01],\n",
      "       [ 1.71380118e-01, -3.28277946e-02, -6.59147799e-02],\n",
      "       [-4.96546715e-01, -2.97641456e-01, -1.47007644e-01],\n",
      "       [-2.82431155e-01, -2.11768866e-01,  1.47404641e-01],\n",
      "       [-2.55191833e-01,  4.21027929e-01, -1.39049619e-01],\n",
      "       [-1.70513600e-01, -7.00020790e-02,  8.87788832e-02],\n",
      "       [ 3.04871500e-02,  4.08505678e-01, -2.32007012e-01],\n",
      "       [ 9.09766257e-02, -3.02733779e-02,  1.79775447e-01],\n",
      "       [-7.31917024e-02,  3.49327922e-02,  1.93707019e-01],\n",
      "       [-6.39993668e-01, -5.21984696e-02, -1.45104527e-03],\n",
      "       [ 2.88348526e-01,  4.56794381e-01, -2.36880139e-01],\n",
      "       [ 1.63891345e-01,  3.11336815e-01, -4.43298817e-01],\n",
      "       [-4.67854142e-02, -1.13316357e-01, -5.33666611e-02],\n",
      "       [ 2.12610096e-01,  9.75047946e-02,  1.17368966e-01],\n",
      "       [ 2.09816590e-01, -9.47159529e-02, -1.74795359e-01],\n",
      "       [ 2.15871409e-01,  4.31877077e-02,  7.29112923e-02],\n",
      "       [-9.29363668e-02,  2.34476566e-01,  2.54077703e-01],\n",
      "       [-1.57812446e-01, -2.40699649e-01, -3.47198248e-02],\n",
      "       [-3.38123739e-02, -1.70125008e-01, -1.61529511e-01],\n",
      "       [-2.07210451e-01,  6.25672758e-01,  2.92887479e-01],\n",
      "       [ 2.78226435e-02, -1.57924235e-01, -1.79149836e-01],\n",
      "       [-1.05343521e-01, -1.81315780e-01, -1.66482598e-01],\n",
      "       [-9.55121517e-02,  3.82530987e-01,  3.14116478e-02],\n",
      "       [ 1.98201299e-01, -3.34397256e-01, -4.12983507e-01],\n",
      "       [-2.33585536e-02, -4.71389890e-02,  2.88199484e-02],\n",
      "       [-9.16407406e-02, -3.01084459e-01, -1.27689719e-01],\n",
      "       [-5.52165508e-02,  4.20509517e-01,  3.61684710e-01],\n",
      "       [ 2.32111216e-02,  1.25644803e-01,  2.93281466e-01],\n",
      "       [ 1.18840858e-01, -9.58634615e-02, -1.25752658e-01],\n",
      "       [ 6.76249862e-01,  4.49622452e-01,  2.95727104e-01],\n",
      "       [-1.76532298e-01,  6.36065185e-01,  2.81820625e-01],\n",
      "       [ 3.88154447e-01, -7.64354467e-02, -1.53220594e-01],\n",
      "       [-1.39945745e-03, -9.58771110e-02, -6.05014861e-02],\n",
      "       [ 1.63704157e-01, -1.20625913e-01,  2.49106586e-02],\n",
      "       [-1.71265215e-01, -2.04330862e-01,  6.85496032e-02],\n",
      "       [ 1.58158824e-01, -6.21236563e-02, -1.15025014e-01],\n",
      "       [ 3.77712727e-01, -3.21660042e-02, -2.26724744e-01],\n",
      "       [ 2.07913443e-01, -1.40594780e-01, -4.35807407e-02],\n",
      "       [ 3.93342137e-01, -1.47046149e-01, -3.40685487e-01],\n",
      "       [ 3.57236803e-01, -1.57334208e-02, -1.47943050e-01],\n",
      "       [-6.21378422e-02,  5.35726547e-04,  2.93965846e-01],\n",
      "       [-7.59096742e-02,  5.51853478e-02,  2.01202899e-01],\n",
      "       [ 1.28015101e-01, -5.08175611e-01, -2.46636927e-01],\n",
      "       [-2.58734852e-01, -9.51775908e-02, -1.15899742e-01],\n",
      "       [-1.15484655e-01, -2.83307850e-01, -1.12786204e-01],\n",
      "       [ 1.63022265e-01,  1.72607362e-01, -6.01113915e-01],\n",
      "       [ 2.00388730e-01,  2.68134296e-01, -3.61294329e-01],\n",
      "       [ 1.64005727e-01, -1.12853050e-01, -1.74165964e-02],\n",
      "       [-3.33255529e-02, -2.22519219e-01, -2.21934319e-02],\n",
      "       [-4.70584780e-01,  3.64063561e-01, -1.80129647e-01],\n",
      "       [ 9.39853489e-02,  3.87698710e-01, -2.42679894e-01],\n",
      "       [-3.21481317e-01,  5.41649461e-01,  1.09021813e-01],\n",
      "       [ 1.36611134e-01, -9.78958011e-02, -1.05232894e-01],\n",
      "       [ 4.67152327e-01, -2.25312531e-01,  1.90865839e+00],\n",
      "       [ 3.85476589e-01, -8.60741138e-02, -1.03792965e-01],\n",
      "       [ 3.78490061e-01,  4.40264165e-01,  1.33059025e+00],\n",
      "       [-3.69842321e-01,  6.35182858e-02, -4.60587710e-01],\n",
      "       [ 3.42947870e-01, -5.24583459e-02, -2.04751432e-01],\n",
      "       [ 1.71460405e-01,  4.03803200e-01, -2.42155448e-01],\n",
      "       [-7.63450265e-02, -7.58073330e-02,  2.20418185e-01],\n",
      "       [-1.33404255e-01,  6.55494928e-02,  4.91440028e-01],\n",
      "       [ 1.18275136e-01, -1.83457196e-01, -8.14888775e-02],\n",
      "       [ 1.16469562e-02, -1.07066631e-01, -5.20189404e-02],\n",
      "       [ 2.61363626e-01, -1.78245544e-01, -1.01972669e-01],\n",
      "       [ 2.54024714e-01, -2.02138543e-01, -1.64461881e-01],\n",
      "       [ 1.94735900e-01, -1.45202935e-01, -2.24496648e-01],\n",
      "       [ 3.08749557e-01, -2.53409147e-03, -1.37069404e-01],\n",
      "       [ 1.34033814e-01, -1.69620156e-01, -5.19073606e-02],\n",
      "       [-9.54219997e-02, -1.68100357e-01,  1.00019783e-01],\n",
      "       [-4.68946248e-01, -2.41857052e-01, -3.28597724e-02],\n",
      "       [ 3.07722360e-01,  6.10029340e-01,  1.27367109e-01],\n",
      "       [ 3.42299819e-01,  2.19982862e-03, -1.00249767e-01],\n",
      "       [ 2.49745876e-01, -4.71227765e-02, -1.35828733e-01],\n",
      "       [ 2.89991200e-01, -1.30132794e-01, -3.04529786e-01],\n",
      "       [ 2.29609668e-01, -7.22323060e-02, -5.81031740e-02],\n",
      "       [-3.53795290e-02, -1.82950616e-01,  2.39923596e-02],\n",
      "       [ 1.16724968e-02,  8.48049521e-02, -2.88319498e-01],\n",
      "       [-5.26462197e-02, -1.75966442e-01,  4.40694094e-02],\n",
      "       [ 9.20671225e-03, -8.73752236e-02,  1.44890696e-01],\n",
      "       [-3.79878283e-03,  5.90392947e-02,  1.70003504e-01],\n",
      "       [ 2.14812711e-01,  2.22872496e-02,  1.07852519e-02],\n",
      "       [-7.10655153e-02,  5.79416871e-01,  1.83899790e-01],\n",
      "       [ 3.70918930e-01, -1.29276574e-01, -3.60716045e-01],\n",
      "       [ 2.81398684e-01, -2.66899586e-01, -3.52346778e-01],\n",
      "       [-1.20581985e-01, -3.45554948e-02, -2.56395310e-01]], dtype=float32)>, <tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[ -48.02719 ,  -46.44605 ,  -43.39006 ],\n",
      "       [ -59.10108 ,  -66.64557 ,  -68.65386 ],\n",
      "       [ -50.74174 ,  -56.254303,  -57.046375],\n",
      "       [ -74.501564,  -77.06582 ,  -77.14595 ],\n",
      "       [ -42.008144,  -51.517223,  -52.3694  ],\n",
      "       [ -69.56048 ,  -73.66879 ,  -71.63757 ],\n",
      "       [ -57.833595,  -64.528305,  -62.686   ],\n",
      "       [ -45.24918 ,  -40.53196 ,  -37.10339 ],\n",
      "       [ -26.653574,  -28.10929 ,  -28.020275],\n",
      "       [ -67.32986 ,  -69.12013 ,  -64.771614],\n",
      "       [ -73.54865 ,  -76.35116 ,  -74.35106 ],\n",
      "       [ -68.37973 ,  -73.20047 ,  -71.17501 ],\n",
      "       [ -28.522165,  -32.091705,  -33.079468],\n",
      "       [ -46.646557,  -42.533012,  -41.328007],\n",
      "       [ -53.007923,  -52.004093,  -52.290672],\n",
      "       [ -70.53996 ,  -56.922276,  -50.28277 ],\n",
      "       [ -44.379295,  -51.328037,  -50.202293],\n",
      "       [ -97.44433 ,  -91.63247 ,  -87.486824],\n",
      "       [ -76.804924,  -80.77149 ,  -79.340294],\n",
      "       [ -54.68085 ,  -47.877148,  -47.313866],\n",
      "       [ -65.7775  ,  -69.36099 ,  -68.319145],\n",
      "       [ -37.69239 ,  -34.916676,  -34.3624  ],\n",
      "       [ -45.794357,  -56.4119  ,  -56.74813 ],\n",
      "       [ -52.58832 ,  -59.94482 ,  -60.383766],\n",
      "       [ -96.19229 ,  -88.273605,  -85.70867 ],\n",
      "       [ -23.66483 ,  -24.355095,  -24.138119],\n",
      "       [ -40.458332,  -35.312534,  -32.8233  ],\n",
      "       [ -60.59594 ,  -64.261314,  -62.46196 ],\n",
      "       [ -33.42792 ,  -44.274036,  -45.059425],\n",
      "       [ -46.142242,  -51.95374 ,  -49.90718 ],\n",
      "       [ -36.196823,  -46.526638,  -46.773098],\n",
      "       [ -39.02725 ,  -47.472534,  -49.699112],\n",
      "       [ -72.700455,  -75.464386,  -72.97109 ],\n",
      "       [ -63.666153,  -65.68744 ,  -62.948727],\n",
      "       [ -34.306137,  -37.01928 ,  -40.573395],\n",
      "       [ -59.497425,  -62.287037,  -59.619595],\n",
      "       [ -68.711815,  -69.2832  ,  -66.31116 ],\n",
      "       [ -44.90157 ,  -44.89686 ,  -45.510235],\n",
      "       [ -48.44332 ,  -54.029865,  -50.451008],\n",
      "       [ -56.18098 ,  -61.306892,  -60.30416 ],\n",
      "       [ -72.32694 ,  -74.86572 ,  -71.63044 ],\n",
      "       [ -31.8914  ,  -40.723454,  -44.15593 ],\n",
      "       [ -41.31158 ,  -52.066055,  -53.77312 ],\n",
      "       [ -50.40835 ,  -55.70699 ,  -53.863297],\n",
      "       [ -57.644836,  -58.402668,  -56.31117 ],\n",
      "       [ -31.727083,  -34.845818,  -38.473328],\n",
      "       [ -33.494675,  -43.287567,  -41.871357],\n",
      "       [ -57.09263 ,  -61.386963,  -59.716644],\n",
      "       [ -46.523132,  -56.11475 ,  -55.237686],\n",
      "       [ -70.57481 ,  -75.01928 ,  -73.340164],\n",
      "       [ -46.363056,  -52.431793,  -50.89484 ],\n",
      "       [ -33.396095,  -41.067898,  -39.34364 ],\n",
      "       [ -46.262646,  -55.200603,  -53.8083  ],\n",
      "       [ -38.972496,  -45.30217 ,  -42.326645],\n",
      "       [ -32.300606,  -41.30509 ,  -40.184402],\n",
      "       [ -52.68807 ,  -62.437866,  -63.32878 ],\n",
      "       [ -52.652576,  -59.7224  ,  -60.21772 ],\n",
      "       [ -62.45245 ,  -70.17446 ,  -66.27417 ],\n",
      "       [ -67.18551 ,  -67.16518 ,  -65.23936 ],\n",
      "       [ -72.88628 ,  -75.13767 ,  -72.03005 ],\n",
      "       [ -45.89122 ,  -39.123253,  -35.21425 ],\n",
      "       [ -34.21317 ,  -33.204025,  -31.56307 ],\n",
      "       [ -47.529408,  -55.944023,  -54.75476 ],\n",
      "       [ -63.891155,  -69.27945 ,  -67.214516],\n",
      "       [ -69.539665,  -58.46645 ,  -57.080166],\n",
      "       [ -34.668972,  -33.09255 ,  -32.51071 ],\n",
      "       [ -50.470844,  -46.7438  ,  -48.202694],\n",
      "       [ -49.084026,  -55.19116 ,  -53.510063],\n",
      "       [-119.56171 , -143.008   , -143.93446 ],\n",
      "       [ -33.3452  ,  -44.220985,  -43.081768],\n",
      "       [ -92.19952 , -100.533066, -101.75265 ],\n",
      "       [ -79.52492 ,  -66.68132 ,  -62.3175  ],\n",
      "       [ -35.533768,  -43.303543,  -41.61367 ],\n",
      "       [ -30.989775,  -30.302126,  -29.782902],\n",
      "       [ -57.062008,  -65.72477 ,  -65.82664 ],\n",
      "       [ -51.590633,  -63.70656 ,  -66.04692 ],\n",
      "       [ -53.498814,  -60.36503 ,  -58.410534],\n",
      "       [ -56.619118,  -61.436123,  -59.797848],\n",
      "       [ -44.538464,  -53.78222 ,  -52.018337],\n",
      "       [ -47.696022,  -55.390144,  -53.01605 ],\n",
      "       [ -46.153465,  -52.05841 ,  -49.776016],\n",
      "       [ -34.663933,  -42.836155,  -41.74396 ],\n",
      "       [ -51.517097,  -59.22881 ,  -57.56362 ],\n",
      "       [ -63.825073,  -70.19544 ,  -69.047554],\n",
      "       [ -92.35272 ,  -88.96067 ,  -85.819984],\n",
      "       [ -29.261835,  -31.683044,  -32.872593],\n",
      "       [ -32.339737,  -41.78723 ,  -40.980858],\n",
      "       [ -40.956505,  -48.091305,  -46.61264 ],\n",
      "       [ -43.530334,  -48.668667,  -45.859695],\n",
      "       [ -41.479992,  -50.26526 ,  -49.20546 ],\n",
      "       [ -62.798668,  -68.533195,  -66.84531 ],\n",
      "       [ -54.130016,  -51.850395,  -49.32868 ],\n",
      "       [ -62.230774,  -68.31596 ,  -66.85656 ],\n",
      "       [ -54.03772 ,  -62.63137 ,  -62.287956],\n",
      "       [ -48.287605,  -56.12622 ,  -56.594917],\n",
      "       [ -38.07421 ,  -47.185333,  -46.921375],\n",
      "       [ -28.906052,  -32.415604,  -35.357224],\n",
      "       [ -39.969643,  -45.281563,  -42.204304],\n",
      "       [ -50.47413 ,  -55.405273,  -51.641777],\n",
      "       [ -60.032192,  -58.699482,  -56.220215]], dtype=float32)>, <tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[ 1.00773662e-01,  2.35963464e-02, -4.27330405e-01],\n",
      "       [-2.83860475e-01,  1.34456813e-01,  4.44695801e-01],\n",
      "       [-1.10274255e-01,  1.48923874e-01,  1.87409967e-01],\n",
      "       [-4.33977574e-01, -1.59469247e-02,  2.61006743e-01],\n",
      "       [ 7.28425980e-02,  8.53597224e-02,  1.83448941e-01],\n",
      "       [-1.33358508e-01, -2.18769133e-01,  1.24764740e-02],\n",
      "       [ 4.93590534e-02, -1.99837923e-01, -3.06572318e-02],\n",
      "       [ 1.68306336e-01,  1.54072315e-01, -5.14794588e-01],\n",
      "       [ 2.08548799e-01,  3.90992433e-01, -2.09065199e-01],\n",
      "       [ 5.74311018e-02, -3.43665123e-01, -3.16123396e-01],\n",
      "       [-2.11883813e-01, -2.01519072e-01,  3.05890143e-02],\n",
      "       [-9.31852758e-02, -2.25590944e-01,  2.24801004e-02],\n",
      "       [ 1.28867418e-01,  4.07469094e-01, -2.99613047e-02],\n",
      "       [-2.23605633e-02,  3.43588471e-01, -2.26695225e-01],\n",
      "       [-2.03532130e-01,  3.14663380e-01,  4.36624289e-02],\n",
      "       [ 4.57111001e-01,  2.85661668e-01, -4.81497675e-01],\n",
      "       [ 1.71380118e-01, -3.28277946e-02, -6.59147799e-02],\n",
      "       [-4.96546715e-01, -2.97641456e-01, -1.47007644e-01],\n",
      "       [-2.82431155e-01, -2.11768866e-01,  1.47404641e-01],\n",
      "       [-2.55191833e-01,  4.21027929e-01, -1.39049619e-01],\n",
      "       [-1.70513600e-01, -7.00020790e-02,  8.87788832e-02],\n",
      "       [ 3.04871537e-02,  4.08505678e-01, -2.32007042e-01],\n",
      "       [ 9.09766257e-02, -3.02733779e-02,  1.79775447e-01],\n",
      "       [-7.31917024e-02,  3.49327922e-02,  1.93707019e-01],\n",
      "       [-6.39993668e-01, -5.21984696e-02, -1.45104527e-03],\n",
      "       [ 2.88347185e-01,  4.56798613e-01, -2.36881122e-01],\n",
      "       [ 1.63891345e-01,  3.11336815e-01, -4.43298757e-01],\n",
      "       [-4.67854142e-02, -1.13316357e-01, -5.33666611e-02],\n",
      "       [ 2.12610126e-01,  9.75047946e-02,  1.17368966e-01],\n",
      "       [ 2.09816590e-01, -9.47159529e-02, -1.74795359e-01],\n",
      "       [ 2.15871394e-01,  4.31877077e-02,  7.29112923e-02],\n",
      "       [-9.29363668e-02,  2.34476566e-01,  2.54077703e-01],\n",
      "       [-1.57812446e-01, -2.40699649e-01, -3.47198248e-02],\n",
      "       [-3.38123739e-02, -1.70125008e-01, -1.61529511e-01],\n",
      "       [-2.07210422e-01,  6.25672758e-01,  2.92887479e-01],\n",
      "       [ 2.78226435e-02, -1.57924235e-01, -1.79149836e-01],\n",
      "       [-1.05343521e-01, -1.81315780e-01, -1.66482598e-01],\n",
      "       [-9.55121517e-02,  3.82530987e-01,  3.14116478e-02],\n",
      "       [ 1.98201299e-01, -3.34397256e-01, -4.12983507e-01],\n",
      "       [-2.33585536e-02, -4.71389890e-02,  2.88199484e-02],\n",
      "       [-9.16407406e-02, -3.01084459e-01, -1.27689719e-01],\n",
      "       [-5.52166067e-02,  4.20509517e-01,  3.61684710e-01],\n",
      "       [ 2.32111216e-02,  1.25644803e-01,  2.93281466e-01],\n",
      "       [ 1.18840858e-01, -9.58634615e-02, -1.25752658e-01],\n",
      "       [ 6.76249862e-01,  4.49622452e-01,  2.95727104e-01],\n",
      "       [-1.76532209e-01,  6.36065185e-01,  2.81820625e-01],\n",
      "       [ 3.88154447e-01, -7.64354467e-02, -1.53220594e-01],\n",
      "       [-1.39945745e-03, -9.58771110e-02, -6.05014861e-02],\n",
      "       [ 1.63704157e-01, -1.20625913e-01,  2.49106586e-02],\n",
      "       [-1.71265215e-01, -2.04330862e-01,  6.85496032e-02],\n",
      "       [ 1.58158824e-01, -6.21236563e-02, -1.15025014e-01],\n",
      "       [ 3.77712697e-01, -3.21660042e-02, -2.26724744e-01],\n",
      "       [ 2.07913443e-01, -1.40594780e-01, -4.35807407e-02],\n",
      "       [ 3.93342137e-01, -1.47046149e-01, -3.40685487e-01],\n",
      "       [ 3.57236683e-01, -1.57334208e-02, -1.47943050e-01],\n",
      "       [-6.21378422e-02,  5.35726547e-04,  2.93965846e-01],\n",
      "       [-7.59096742e-02,  5.51853478e-02,  2.01202899e-01],\n",
      "       [ 1.28015101e-01, -5.08175611e-01, -2.46636927e-01],\n",
      "       [-2.58734852e-01, -9.51775908e-02, -1.15899742e-01],\n",
      "       [-1.15484655e-01, -2.83307850e-01, -1.12786204e-01],\n",
      "       [ 1.63022265e-01,  1.72607362e-01, -6.01113915e-01],\n",
      "       [ 2.00388744e-01,  2.68134236e-01, -3.61294299e-01],\n",
      "       [ 1.64005727e-01, -1.12853050e-01, -1.74165964e-02],\n",
      "       [-3.33255529e-02, -2.22519219e-01, -2.21934319e-02],\n",
      "       [-4.70584780e-01,  3.64063561e-01, -1.80129647e-01],\n",
      "       [ 9.39853787e-02,  3.87698770e-01, -2.42679894e-01],\n",
      "       [-3.21481317e-01,  5.41649461e-01,  1.09021813e-01],\n",
      "       [ 1.36611134e-01, -9.78958011e-02, -1.05232894e-01],\n",
      "       [ 4.67152327e-01, -2.25312531e-01,  1.90865839e+00],\n",
      "       [ 3.85476679e-01, -8.60741138e-02, -1.03792965e-01],\n",
      "       [ 3.78490061e-01,  4.40264165e-01,  1.33059025e+00],\n",
      "       [-3.69842321e-01,  6.35182858e-02, -4.60587710e-01],\n",
      "       [ 3.42947900e-01, -5.24583459e-02, -2.04751432e-01],\n",
      "       [ 1.71460509e-01,  4.03802812e-01, -2.42155463e-01],\n",
      "       [-7.63450265e-02, -7.58073330e-02,  2.20418185e-01],\n",
      "       [-1.33404255e-01,  6.55494928e-02,  4.91440028e-01],\n",
      "       [ 1.18275136e-01, -1.83457196e-01, -8.14888775e-02],\n",
      "       [ 1.16469562e-02, -1.07066631e-01, -5.20189404e-02],\n",
      "       [ 2.61363626e-01, -1.78245544e-01, -1.01972669e-01],\n",
      "       [ 2.54024714e-01, -2.02138543e-01, -1.64461881e-01],\n",
      "       [ 1.94735900e-01, -1.45202935e-01, -2.24496648e-01],\n",
      "       [ 3.08749586e-01, -2.53409147e-03, -1.37069404e-01],\n",
      "       [ 1.34033814e-01, -1.69620156e-01, -5.19073606e-02],\n",
      "       [-9.54219997e-02, -1.68100357e-01,  1.00019783e-01],\n",
      "       [-4.68946248e-01, -2.41857052e-01, -3.28597724e-02],\n",
      "       [ 3.07723075e-01,  6.10029519e-01,  1.27367124e-01],\n",
      "       [ 3.42299789e-01,  2.19982886e-03, -1.00249767e-01],\n",
      "       [ 2.49745876e-01, -4.71227765e-02, -1.35828733e-01],\n",
      "       [ 2.89991200e-01, -1.30132794e-01, -3.04529786e-01],\n",
      "       [ 2.29609668e-01, -7.22323060e-02, -5.81031740e-02],\n",
      "       [-3.53795290e-02, -1.82950616e-01,  2.39923596e-02],\n",
      "       [ 1.16724968e-02,  8.48049521e-02, -2.88319498e-01],\n",
      "       [-5.26462197e-02, -1.75966442e-01,  4.40694094e-02],\n",
      "       [ 9.20671225e-03, -8.73752236e-02,  1.44890696e-01],\n",
      "       [-3.79878283e-03,  5.90392947e-02,  1.70003504e-01],\n",
      "       [ 2.14812711e-01,  2.22872496e-02,  1.07852519e-02],\n",
      "       [-7.10657835e-02,  5.79416692e-01,  1.83899745e-01],\n",
      "       [ 3.70918930e-01, -1.29276574e-01, -3.60716045e-01],\n",
      "       [ 2.81398684e-01, -2.66899586e-01, -3.52346778e-01],\n",
      "       [-1.20581985e-01, -3.45554948e-02, -2.56395310e-01]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "a=Data_Train_Flat[0:100]\n",
    "b=vae_40MHZ.encoder(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67750e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
