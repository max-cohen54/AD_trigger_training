{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 00:25:48.295621: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-14 00:25:51.344575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_AE(input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(z)\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(z)\n",
    "    outputs = layers.Dense(input_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 57)]              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                1856      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                288       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 57)                1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,689\n",
      "Trainable params: 4,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INPUT_DIM = Data_Train_Flat.shape[1]\n",
    "H_DIM_1 = 32\n",
    "H_DIM_2 = 16\n",
    "LATENT_DIM = 8\n",
    "DNN_AE = create_AE(input_dim=INPUT_DIM, h_dim_1=H_DIM_1, h_dim_2=H_DIM_2, latent_dim=LATENT_DIM)\n",
    "DNN_AE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1954/1954 [==============================] - 11s 4ms/step - loss: 0.0193 - val_loss: 0.0056 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0051 - val_loss: 0.0048 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0045 - val_loss: 0.0045 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0043 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0041 - val_loss: 0.0039 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0039 - val_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0038 - val_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0038 - val_loss: 0.0037 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0037 - val_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0037 - val_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0036 - val_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0036 - val_loss: 0.0036 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0036 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0035 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0035 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0035 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0034 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0034 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0034 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0034 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "1947/1954 [============================>.] - ETA: 0s - loss: 0.0034\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0034 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "1951/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "1945/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-06\n",
      "Epoch 32/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-06\n",
      "Epoch 33/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-06\n",
      "Epoch 34/50\n",
      "1952/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-06\n",
      "Epoch 35/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-07\n",
      "Epoch 36/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-07\n",
      "Epoch 37/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-07\n",
      "Epoch 38/50\n",
      "1947/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-07\n",
      "Epoch 39/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-08\n",
      "Epoch 40/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-08\n",
      "Epoch 41/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-08\n",
      "Epoch 42/50\n",
      "1947/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-08\n",
      "Epoch 43/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-09\n",
      "Epoch 44/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-09\n",
      "Epoch 45/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-09\n",
      "Epoch 46/50\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-09\n",
      "Epoch 47/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-10\n",
      "Epoch 48/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-10\n",
      "Epoch 49/50\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-10\n",
      "Epoch 50/50\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 0.0033\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 0.0033 - val_loss: 0.0032 - lr: 1.0000e-10\n"
     ]
    }
   ],
   "source": [
    "DNN_AE.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "history = DNN_AE.fit(x=Data_Train_Flat, y=Data_Train_Flat, validation_data=(Data_Val_Flat,Data_Val_Flat), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_AE.save('/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
