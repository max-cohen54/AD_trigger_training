{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 22:16:03.910438: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 22:16:03.978417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.math as tfmath\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_VAE_Model(Model):\n",
    "    def __init__(self,input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "        super(DNN_VAE_Model,self).__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.e1=layers.Dense(h_dim_1, activation='relu')\n",
    "        self.e2=layers.Dense(h_dim_2, activation='relu')\n",
    "        self.mean=layers.Dense(latent_dim, activation='relu')\n",
    "        self.logvar=layers.Dense(latent_dim, activation='relu')\n",
    "\n",
    "        self.d1=layers.Dense(h_dim_2, activation='relu')\n",
    "        self.d2=layers.Dense(h_dim_1, activation='relu')\n",
    "        self.d3=layers.Dense(input_dim)\n",
    "        \n",
    "    def sample(self,m,logvar):\n",
    "        std=tfmath.exp(0.5*logvar)\n",
    "        eps=tf.random.normal(tf.shape(m))\n",
    "        x=m+eps*std\n",
    "        return x\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x= self.e1(inputs)\n",
    "        x= self.e2(x)\n",
    "        m=self.mean(x)\n",
    "        logvar=self.logvar(x)\n",
    "        self.m=m\n",
    "        self.lv=logvar\n",
    "\n",
    "        x=self.sample(m,logvar)\n",
    "\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        z=self.d3(x)\n",
    "        return z\n",
    "    \n",
    "    def loss_function(self,y_true,y_pred,beta=1):\n",
    "        \"\"\"masked mse\"\"\"\n",
    "        mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "        squared_difference = K.square(mask * (y_pred - y_true))\n",
    "        L1= K.mean(squared_difference)\n",
    "\n",
    "        mu=self.m\n",
    "        logv=self.lv\n",
    "\n",
    "        KL=-0.5*K.mean(1+logv-mu**2 - tfmath.exp(logv))\n",
    "\n",
    "        loss=L1+beta*KL\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)\n",
    "\n",
    "class KLDivergenceLayer(keras.layers.Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs,beta):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(beta*K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DNN_VAE\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 57)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_133 (Dense)              (None, 32)           1856        ['input_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_134 (Dense)              (None, 16)           528         ['dense_133[0][0]']              \n",
      "                                                                                                  \n",
      " dense_135 (Dense)              (None, 8)            136         ['dense_134[0][0]']              \n",
      "                                                                                                  \n",
      " dense_136 (Dense)              (None, 8)            136         ['dense_134[0][0]']              \n",
      "                                                                                                  \n",
      " kl_divergence_layer (KLDiverge  [(None, 8),         0           ['dense_135[0][0]',              \n",
      " nceLayer)                       (None, 8)]                       'dense_136[0][0]']              \n",
      "                                                                                                  \n",
      " lambda_3 (Lambda)              (None, 8)            0           ['kl_divergence_layer[0][0]',    \n",
      "                                                                  'kl_divergence_layer[0][1]']    \n",
      "                                                                                                  \n",
      " dense_137 (Dense)              (None, 16)           144         ['lambda_3[0][0]']               \n",
      "                                                                                                  \n",
      " dense_138 (Dense)              (None, 32)           544         ['dense_137[0][0]']              \n",
      "                                                                                                  \n",
      " dense_139 (Dense)              (None, 57)           1881        ['dense_138[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,225\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_dim = 57\n",
    "h_dim_1 = 32\n",
    "h_dim_2 = 16\n",
    "latent_dim = 8\n",
    "beta=1\n",
    "inputs=keras.Input(shape=(input_dim,))\n",
    "x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "m=layers.Dense(latent_dim, activation='relu')(x)\n",
    "logvar=layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "def sample(args):\n",
    "    m,logvar=args\n",
    "    std=tfmath.exp(0.5*logvar)\n",
    "    eps=tf.random.normal(tf.shape(m))\n",
    "    x=m+eps*std\n",
    "    return x\n",
    "\n",
    "m,logvar=KLDivergenceLayer()([m,logvar],1)\n",
    "\n",
    "x=layers.Lambda(sample,output_shape=(input_dim,))([m,logvar])\n",
    "x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "outputs=layers.Dense(input_dim)(x)\n",
    "\n",
    "DNN_VAE = keras.Model(inputs=inputs, outputs=outputs, name=\"DNN_VAE\")\n",
    "DNN_VAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 57)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        [(None, 8),               2656      \n",
      "                              (None, 8),                         \n",
      "                              (None, 8)]                         \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 57)                2569      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1954/1954 [==============================] - 17s 7ms/step - loss: 0.2164 - val_loss: 0.2154 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1954/1954 [==============================] - 13s 7ms/step - loss: 0.2156 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2156 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2156 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1946/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1954/1954 [==============================] - 14s 7ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "1954/1954 [==============================] - ETA: 0s - loss: 0.2155\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "1949/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 15/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 16/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 17/50\n",
      "1949/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 19/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 20/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 21/50\n",
      "1950/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 22/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 23/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 24/50\n",
      "1954/1954 [==============================] - 12s 6ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n"
     ]
    }
   ],
   "source": [
    "input_dim = 57\n",
    "h_dim_1 = 32\n",
    "h_dim_2 = 16\n",
    "latent_dim = 8\n",
    "beta=1\n",
    "inputs=keras.Input(shape=(input_dim,))\n",
    "x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "z_mean=layers.Dense(latent_dim, activation='relu')(x)\n",
    "z_logvar=layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "def sample(args):\n",
    "    m,logvar=args\n",
    "    std=tfmath.exp(0.5*logvar)\n",
    "    eps=tf.random.normal(tf.shape(m))\n",
    "    x=m+eps*std\n",
    "    return x\n",
    "\n",
    "z=layers.Lambda(sample)([z_mean,z_logvar])\n",
    "\n",
    "encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "\n",
    "latent_inputs=keras.Input(shape=(latent_dim,),name='z_sampling')\n",
    "x=layers.Dense(h_dim_2, activation='relu')(latent_inputs)\n",
    "x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "outputs=layers.Dense(input_dim)(x)\n",
    "decoder=keras.Model(latent_inputs,outputs,name='decoder')\n",
    "\n",
    "outputs=decoder(encoder(inputs)[2])\n",
    "vae=keras.Model(inputs,outputs,name='vae')\n",
    "vae.summary()\n",
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)\n",
    "\n",
    "def KL_func(mu,logv):\n",
    "    KL=-0.5*K.mean(1+logv-mu**2 - tfmath.exp(logv))\n",
    "    loss=KL\n",
    "    return loss\n",
    "    \n",
    "recon_loss=loss_fn(inputs,outputs)\n",
    "KL_loss=KL_func(z_mean,z_logvar)\n",
    "vae_loss=recon_loss+beta*KL_loss\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "history = vae.fit(x=Data_Train_Flat, y=Data_Train_Flat, validation_data=(Data_Val_Flat,Data_Val_Flat), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Found duplicated `Variable`s in Model's `weights`. This is usually caused by `Variable`s being shared by Layers in the Model. These `Variable`s will be treated as separate `Variable`s when the Model is restored. To avoid this, please save with `save_format=\"tf\"`.\n"
     ]
    }
   ],
   "source": [
    "vae.save('/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN_VAE.keras',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vae2=keras.saving.load_model('/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN_VAE.keras',compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25586/399999 [>.............................] - ETA: 10:59 - loss: 0.2153"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_552/753714807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLossHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvae2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData_Val_Flat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2065\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2068\u001b[0m                         with tf.profiler.experimental.Trace(\n\u001b[1;32m   2069\u001b[0m                             \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0moriginal_spe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             can_run_full_execution = (\n\u001b[1;32m   1377\u001b[0m                 \u001b[0moriginal_spe\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    649\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \"\"\"\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m     \u001b[0;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_read_variable_op\u001b[0;34m(self, no_copy)\u001b[0m\n\u001b[1;32m    736\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m     \"\"\"\n\u001b[0;32m--> 738\u001b[0;31m     \u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_and_set_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_104a_swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvariable_accessed\u001b[0;34m(variable)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m     \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_accessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_test_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "history=LossHistory()\n",
    "vae2.evaluate(Data_Val_Flat,batch_size=1,callbacks=[history])\n",
    "print(history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
