{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c66c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-02 00:03:52.053915: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-02 00:03:52.143735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfmath\n",
    "import tensorflow.keras as keras\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20999b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "#for i, batch in enumerate(Dataset):\n",
    "#  pt_sum=0\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    if particle[3]!=0:\n",
    "#      pt_sum+=particle[0]\n",
    "#  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "#    particle[0]=particle[0]/pt_sum\n",
    "    \n",
    "    \n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]\n",
    "\n",
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591db785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def make_encoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(input_dim))\n",
    "    #x=layers.BatchNormalization()(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    z_mean=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z_logvar=layers.Dense(latent_dim, activation='relu')(x)\n",
    "    z=Sampling()([z_mean,z_logvar])\n",
    "    encoder=keras.Model(inputs,[z_mean,z_logvar,z],name='encoder')\n",
    "    return encoder\n",
    "\n",
    "\n",
    "#from v2 to v3 I removed the activation function from the last layer\n",
    "def make_decoder(input_dim,h_dim_1,h_dim_2,latent_dim):\n",
    "    inputs=keras.Input(shape=(latent_dim))\n",
    "    x=layers.Dense(h_dim_2, activation='relu')(inputs)\n",
    "    x=layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    z=layers.Dense(input_dim)(x)\n",
    "    decoder=keras.Model(inputs,z,name='decoder')\n",
    "    return decoder\n",
    "\n",
    "\n",
    "#VAE Model Version 3\n",
    "class VAE_Model(keras.Model):\n",
    "    def __init__(self,encoder,decoder,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.beta=0.5\n",
    "\n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean,z_log_var,x = self.encoder(data)\n",
    "        reconstruction = self.decoder(x)\n",
    "        \n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        \n",
    "        mask = K.cast(K.not_equal(data, 0), K.floatx())\n",
    "        \n",
    "        MSE_loss=tf.reduce_mean(keras.losses.mse(mask*data, mask*reconstruction))\n",
    "        \n",
    "        loss=self.beta*kl_loss+(1-self.beta)*MSE_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(loss)\n",
    "        self.reconstruction_loss_tracker.update_state((1-self.beta)*MSE_loss)\n",
    "        self.kl_loss_tracker.update_state(self.beta*kl_loss)\n",
    "        \n",
    "        self.add_loss(loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        return {\n",
    "            \"z_mean\": z_mean,\n",
    "            \"z_log_var\": z_log_var,\n",
    "            \"reconstruction\": reconstruction\n",
    "        }\n",
    "    \n",
    "    def set_beta(self,beta):\n",
    "        self.beta=beta\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f8c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1954/1954 [==============================] - 11s 5ms/step - loss: 449278.6562 - total_loss: 449278.6562 - reconstruction_loss: 6112.7173 - kl_loss: 443169.5938 - val_loss: 2.3817 - val_total_loss: 2.3817 - val_reconstruction_loss: 2.0823 - val_kl_loss: 0.2994 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.2585 - total_loss: 2.2585 - reconstruction_loss: 1.9717 - kl_loss: 0.2868 - val_loss: 2.2354 - val_total_loss: 2.2354 - val_reconstruction_loss: 1.9534 - val_kl_loss: 0.2821 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.1935 - total_loss: 2.1935 - reconstruction_loss: 1.9135 - kl_loss: 0.2800 - val_loss: 2.1940 - val_total_loss: 2.1940 - val_reconstruction_loss: 1.9233 - val_kl_loss: 0.2707 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1671 - total_loss: 2.1671 - reconstruction_loss: 1.8876 - kl_loss: 0.2795 - val_loss: 2.1792 - val_total_loss: 2.1792 - val_reconstruction_loss: 1.8922 - val_kl_loss: 0.2870 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1581 - total_loss: 2.1581 - reconstruction_loss: 1.8758 - kl_loss: 0.2823 - val_loss: 2.1734 - val_total_loss: 2.1734 - val_reconstruction_loss: 1.8680 - val_kl_loss: 0.3054 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1516 - total_loss: 2.1516 - reconstruction_loss: 1.8679 - kl_loss: 0.2837 - val_loss: 2.1783 - val_total_loss: 2.1783 - val_reconstruction_loss: 1.8470 - val_kl_loss: 0.3313 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1463 - total_loss: 2.1463 - reconstruction_loss: 1.8608 - kl_loss: 0.2855 - val_loss: 2.1629 - val_total_loss: 2.1629 - val_reconstruction_loss: 1.8912 - val_kl_loss: 0.2717 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.1396 - total_loss: 2.1396 - reconstruction_loss: 1.8531 - kl_loss: 0.2866 - val_loss: 2.1513 - val_total_loss: 2.1513 - val_reconstruction_loss: 1.8668 - val_kl_loss: 0.2845 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1314 - total_loss: 2.1314 - reconstruction_loss: 1.8425 - kl_loss: 0.2889 - val_loss: 2.1408 - val_total_loss: 2.1408 - val_reconstruction_loss: 1.8647 - val_kl_loss: 0.2761 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "1954/1954 [==============================] - 9s 5ms/step - loss: 2.1188 - total_loss: 2.1188 - reconstruction_loss: 1.8259 - kl_loss: 0.2929 - val_loss: 2.1666 - val_total_loss: 2.1666 - val_reconstruction_loss: 1.9385 - val_kl_loss: 0.2281 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1086 - total_loss: 2.1086 - reconstruction_loss: 1.8125 - kl_loss: 0.2961 - val_loss: 2.1162 - val_total_loss: 2.1162 - val_reconstruction_loss: 1.8136 - val_kl_loss: 0.3027 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1033 - total_loss: 2.1033 - reconstruction_loss: 1.8049 - kl_loss: 0.2984 - val_loss: 2.1156 - val_total_loss: 2.1156 - val_reconstruction_loss: 1.8367 - val_kl_loss: 0.2789 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.1005 - total_loss: 2.1005 - reconstruction_loss: 1.8002 - kl_loss: 0.3003 - val_loss: 2.1111 - val_total_loss: 2.1111 - val_reconstruction_loss: 1.8177 - val_kl_loss: 0.2934 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0974 - total_loss: 2.0974 - reconstruction_loss: 1.7967 - kl_loss: 0.3007 - val_loss: 2.1221 - val_total_loss: 2.1221 - val_reconstruction_loss: 1.8615 - val_kl_loss: 0.2606 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0952 - total_loss: 2.0952 - reconstruction_loss: 1.7936 - kl_loss: 0.3017 - val_loss: 2.1100 - val_total_loss: 2.1100 - val_reconstruction_loss: 1.8280 - val_kl_loss: 0.2820 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0910 - total_loss: 2.0910 - reconstruction_loss: 1.7891 - kl_loss: 0.3018 - val_loss: 2.1016 - val_total_loss: 2.1016 - val_reconstruction_loss: 1.8173 - val_kl_loss: 0.2843 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0905 - total_loss: 2.0905 - reconstruction_loss: 1.7868 - kl_loss: 0.3037 - val_loss: 2.1017 - val_total_loss: 2.1017 - val_reconstruction_loss: 1.8004 - val_kl_loss: 0.3012 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0897 - total_loss: 2.0897 - reconstruction_loss: 1.7852 - kl_loss: 0.3045 - val_loss: 2.1046 - val_total_loss: 2.1046 - val_reconstruction_loss: 1.7931 - val_kl_loss: 0.3115 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0887 - total_loss: 2.0887 - reconstruction_loss: 1.7828 - kl_loss: 0.3059 - val_loss: 2.1154 - val_total_loss: 2.1154 - val_reconstruction_loss: 1.8432 - val_kl_loss: 0.2721 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0852 - total_loss: 2.0852 - reconstruction_loss: 1.7779 - kl_loss: 0.3073 - val_loss: 2.0940 - val_total_loss: 2.0940 - val_reconstruction_loss: 1.7892 - val_kl_loss: 0.3049 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0842 - total_loss: 2.0842 - reconstruction_loss: 1.7756 - kl_loss: 0.3086 - val_loss: 2.1036 - val_total_loss: 2.1036 - val_reconstruction_loss: 1.7737 - val_kl_loss: 0.3299 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0835 - total_loss: 2.0835 - reconstruction_loss: 1.7747 - kl_loss: 0.3088 - val_loss: 2.1017 - val_total_loss: 2.1017 - val_reconstruction_loss: 1.7538 - val_kl_loss: 0.3479 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0825 - total_loss: 2.0825 - reconstruction_loss: 1.7722 - kl_loss: 0.3103 - val_loss: 2.0900 - val_total_loss: 2.0900 - val_reconstruction_loss: 1.7847 - val_kl_loss: 0.3053 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0817 - total_loss: 2.0817 - reconstruction_loss: 1.7702 - kl_loss: 0.3116 - val_loss: 2.0927 - val_total_loss: 2.0927 - val_reconstruction_loss: 1.7846 - val_kl_loss: 0.3082 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0794 - total_loss: 2.0794 - reconstruction_loss: 1.7674 - kl_loss: 0.3120 - val_loss: 2.0930 - val_total_loss: 2.0930 - val_reconstruction_loss: 1.7981 - val_kl_loss: 0.2950 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0797 - total_loss: 2.0797 - reconstruction_loss: 1.7664 - kl_loss: 0.3133 - val_loss: 2.0890 - val_total_loss: 2.0890 - val_reconstruction_loss: 1.7567 - val_kl_loss: 0.3323 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "1954/1954 [==============================] - 9s 5ms/step - loss: 2.0753 - total_loss: 2.0753 - reconstruction_loss: 1.7615 - kl_loss: 0.3138 - val_loss: 2.0864 - val_total_loss: 2.0864 - val_reconstruction_loss: 1.7719 - val_kl_loss: 0.3145 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0745 - total_loss: 2.0745 - reconstruction_loss: 1.7587 - kl_loss: 0.3157 - val_loss: 2.0883 - val_total_loss: 2.0883 - val_reconstruction_loss: 1.7828 - val_kl_loss: 0.3056 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0707 - total_loss: 2.0707 - reconstruction_loss: 1.7545 - kl_loss: 0.3162 - val_loss: 2.0867 - val_total_loss: 2.0867 - val_reconstruction_loss: 1.7661 - val_kl_loss: 0.3206 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0710 - total_loss: 2.0710 - reconstruction_loss: 1.7544 - kl_loss: 0.3166 - val_loss: 2.0853 - val_total_loss: 2.0853 - val_reconstruction_loss: 1.7816 - val_kl_loss: 0.3037 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0713 - total_loss: 2.0713 - reconstruction_loss: 1.7544 - kl_loss: 0.3168 - val_loss: 2.0857 - val_total_loss: 2.0857 - val_reconstruction_loss: 1.7855 - val_kl_loss: 0.3002 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0693 - total_loss: 2.0693 - reconstruction_loss: 1.7514 - kl_loss: 0.3179 - val_loss: 2.0859 - val_total_loss: 2.0859 - val_reconstruction_loss: 1.7815 - val_kl_loss: 0.3044 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0686 - total_loss: 2.0686 - reconstruction_loss: 1.7510 - kl_loss: 0.3176 - val_loss: 2.0834 - val_total_loss: 2.0834 - val_reconstruction_loss: 1.7509 - val_kl_loss: 0.3325 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0672 - total_loss: 2.0672 - reconstruction_loss: 1.7485 - kl_loss: 0.3186 - val_loss: 2.0791 - val_total_loss: 2.0791 - val_reconstruction_loss: 1.7680 - val_kl_loss: 0.3112 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0674 - total_loss: 2.0674 - reconstruction_loss: 1.7488 - kl_loss: 0.3186 - val_loss: 2.0838 - val_total_loss: 2.0838 - val_reconstruction_loss: 1.7636 - val_kl_loss: 0.3203 - lr: 0.0010\n",
      "Epoch 36/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0678 - total_loss: 2.0678 - reconstruction_loss: 1.7480 - kl_loss: 0.3198 - val_loss: 2.0820 - val_total_loss: 2.0820 - val_reconstruction_loss: 1.7486 - val_kl_loss: 0.3334 - lr: 0.0010\n",
      "Epoch 37/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0654 - total_loss: 2.0654 - reconstruction_loss: 1.7462 - kl_loss: 0.3192 - val_loss: 2.0813 - val_total_loss: 2.0813 - val_reconstruction_loss: 1.7317 - val_kl_loss: 0.3496 - lr: 0.0010\n",
      "Epoch 38/100\n",
      "1954/1954 [==============================] - ETA: 0s - loss: 2.0673 - total_loss: 2.0673 - reconstruction_loss: 1.7479 - kl_loss: 0.3194\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0673 - total_loss: 2.0673 - reconstruction_loss: 1.7479 - kl_loss: 0.3194 - val_loss: 2.0800 - val_total_loss: 2.0800 - val_reconstruction_loss: 1.7471 - val_kl_loss: 0.3328 - lr: 0.0010\n",
      "Epoch 39/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0603 - total_loss: 2.0603 - reconstruction_loss: 1.7403 - kl_loss: 0.3200 - val_loss: 2.0758 - val_total_loss: 2.0758 - val_reconstruction_loss: 1.7432 - val_kl_loss: 0.3325 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0605 - total_loss: 2.0605 - reconstruction_loss: 1.7389 - kl_loss: 0.3216 - val_loss: 2.0712 - val_total_loss: 2.0712 - val_reconstruction_loss: 1.7435 - val_kl_loss: 0.3277 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0589 - total_loss: 2.0589 - reconstruction_loss: 1.7368 - kl_loss: 0.3220 - val_loss: 2.0717 - val_total_loss: 2.0717 - val_reconstruction_loss: 1.7513 - val_kl_loss: 0.3204 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0589 - total_loss: 2.0589 - reconstruction_loss: 1.7366 - kl_loss: 0.3223 - val_loss: 2.0726 - val_total_loss: 2.0726 - val_reconstruction_loss: 1.7619 - val_kl_loss: 0.3107 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0597 - total_loss: 2.0597 - reconstruction_loss: 1.7372 - kl_loss: 0.3225 - val_loss: 2.0727 - val_total_loss: 2.0727 - val_reconstruction_loss: 1.7458 - val_kl_loss: 0.3269 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "1948/1954 [============================>.] - ETA: 0s - loss: 2.0583 - total_loss: 2.0583 - reconstruction_loss: 1.7357 - kl_loss: 0.3225\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0587 - total_loss: 2.0587 - reconstruction_loss: 1.7361 - kl_loss: 0.3226 - val_loss: 2.0735 - val_total_loss: 2.0735 - val_reconstruction_loss: 1.7465 - val_kl_loss: 0.3270 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0590 - total_loss: 2.0590 - reconstruction_loss: 1.7359 - kl_loss: 0.3231 - val_loss: 2.0734 - val_total_loss: 2.0734 - val_reconstruction_loss: 1.7517 - val_kl_loss: 0.3217 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0578 - total_loss: 2.0578 - reconstruction_loss: 1.7346 - kl_loss: 0.3232 - val_loss: 2.0724 - val_total_loss: 2.0724 - val_reconstruction_loss: 1.7477 - val_kl_loss: 0.3247 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0584 - total_loss: 2.0584 - reconstruction_loss: 1.7359 - kl_loss: 0.3225 - val_loss: 2.0758 - val_total_loss: 2.0758 - val_reconstruction_loss: 1.7538 - val_kl_loss: 0.3220 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0585 - total_loss: 2.0585 - reconstruction_loss: 1.7356 - kl_loss: 0.3228 - val_loss: 2.0704 - val_total_loss: 2.0704 - val_reconstruction_loss: 1.7468 - val_kl_loss: 0.3236 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0576 - total_loss: 2.0576 - reconstruction_loss: 1.7346 - kl_loss: 0.3229 - val_loss: 2.0712 - val_total_loss: 2.0712 - val_reconstruction_loss: 1.7482 - val_kl_loss: 0.3230 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0587 - total_loss: 2.0587 - reconstruction_loss: 1.7358 - kl_loss: 0.3229 - val_loss: 2.0740 - val_total_loss: 2.0740 - val_reconstruction_loss: 1.7473 - val_kl_loss: 0.3267 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0586 - total_loss: 2.0586 - reconstruction_loss: 1.7347 - kl_loss: 0.3239 - val_loss: 2.0689 - val_total_loss: 2.0689 - val_reconstruction_loss: 1.7433 - val_kl_loss: 0.3256 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0564 - total_loss: 2.0564 - reconstruction_loss: 1.7337 - kl_loss: 0.3227 - val_loss: 2.0707 - val_total_loss: 2.0707 - val_reconstruction_loss: 1.7490 - val_kl_loss: 0.3217 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0594 - total_loss: 2.0594 - reconstruction_loss: 1.7356 - kl_loss: 0.3237 - val_loss: 2.0736 - val_total_loss: 2.0736 - val_reconstruction_loss: 1.7514 - val_kl_loss: 0.3221 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0578 - total_loss: 2.0578 - reconstruction_loss: 1.7354 - kl_loss: 0.3225 - val_loss: 2.0720 - val_total_loss: 2.0720 - val_reconstruction_loss: 1.7490 - val_kl_loss: 0.3230 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "1953/1954 [============================>.] - ETA: 0s - loss: 2.0581 - total_loss: 2.0581 - reconstruction_loss: 1.7353 - kl_loss: 0.3227\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0580 - total_loss: 2.0580 - reconstruction_loss: 1.7353 - kl_loss: 0.3227 - val_loss: 2.0729 - val_total_loss: 2.0729 - val_reconstruction_loss: 1.7474 - val_kl_loss: 0.3255 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0587 - total_loss: 2.0587 - reconstruction_loss: 1.7350 - kl_loss: 0.3237 - val_loss: 2.0722 - val_total_loss: 2.0722 - val_reconstruction_loss: 1.7487 - val_kl_loss: 0.3234 - lr: 1.0000e-06\n",
      "Epoch 57/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0588 - total_loss: 2.0588 - reconstruction_loss: 1.7361 - kl_loss: 0.3226 - val_loss: 2.0724 - val_total_loss: 2.0724 - val_reconstruction_loss: 1.7497 - val_kl_loss: 0.3227 - lr: 1.0000e-06\n",
      "Epoch 58/100\n",
      "1954/1954 [==============================] - 9s 4ms/step - loss: 2.0575 - total_loss: 2.0575 - reconstruction_loss: 1.7348 - kl_loss: 0.3227 - val_loss: 2.0732 - val_total_loss: 2.0732 - val_reconstruction_loss: 1.7503 - val_kl_loss: 0.3229 - lr: 1.0000e-06\n",
      "Epoch 59/100\n",
      "1944/1954 [============================>.] - ETA: 0s - loss: 2.0590 - total_loss: 2.0590 - reconstruction_loss: 1.7364 - kl_loss: 0.3226\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1954/1954 [==============================] - 8s 4ms/step - loss: 2.0586 - total_loss: 2.0586 - reconstruction_loss: 1.7361 - kl_loss: 0.3225 - val_loss: 2.0739 - val_total_loss: 2.0739 - val_reconstruction_loss: 1.7510 - val_kl_loss: 0.3230 - lr: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "#Here is a normalized model with (1-beta)rl beta*Kl loss\n",
    "beta=0.83\n",
    "vae_enc=make_encoder(57,32,16,3)\n",
    "vae_dec=make_decoder(57,32,16,3)\n",
    "vae_40MHZ=VAE_Model(vae_enc,vae_dec)\n",
    "vae_40MHZ.set_beta(beta)\n",
    "opt=keras.optimizers.Adam()\n",
    "vae_40MHZ.compile(optimizer=opt)\n",
    "\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "history = vae_40MHZ.fit(x=Data_Train_Flat, validation_data=(Data_Val_Flat,Data_Val_Flat),epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks,shuffle=True)\n",
    "vae_40MHZ.save_weights(filepath='/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/Different_VAE_Models/non_normed_new_beta_{}_v3/'.format(beta),save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93fb8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.        ],\n",
      "       [0.        , 3.1305077 , 0.        ],\n",
      "       [0.        , 2.2455888 , 0.        ],\n",
      "       [0.        , 3.0734463 , 0.        ],\n",
      "       [0.        , 1.3216188 , 0.        ],\n",
      "       [0.        , 0.60316646, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.98985153, 0.        ],\n",
      "       [0.        , 0.566447  , 0.        ],\n",
      "       [0.        , 0.04169333, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.2586201 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 2.9763327 , 0.        ],\n",
      "       [0.        , 1.8671203 , 0.        ],\n",
      "       [0.        , 0.34932318, 0.        ],\n",
      "       [0.        , 1.407789  , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.0899816 , 0.        ],\n",
      "       [0.        , 1.7051702 , 0.        ],\n",
      "       [0.        , 2.8500452 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.09353065, 0.        ],\n",
      "       [0.        , 0.5210673 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.17814611, 0.        ],\n",
      "       [0.        , 2.4168434 , 0.        ],\n",
      "       [0.        , 0.3841352 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.8460863 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.96726376, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.59344536, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 2.120648  , 0.        ],\n",
      "       [0.        , 1.9710171 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.1198463 , 0.        ],\n",
      "       [0.        , 1.7776747 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.0636287 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 2.2047503 , 0.        ],\n",
      "       [0.        , 1.8882067 , 0.        ],\n",
      "       [0.        , 1.3867433 , 0.        ],\n",
      "       [0.        , 1.6074562 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.06838799, 0.        ],\n",
      "       [0.        , 0.7436505 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.6125975 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 6.3381352 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 3.9985065 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.7107775 , 0.        ],\n",
      "       [0.        , 2.9051015 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.040238  , 0.        ],\n",
      "       [0.        , 1.4325233 , 0.        ],\n",
      "       [0.        , 0.17293873, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.43858808, 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.5726408 , 0.        ],\n",
      "       [0.        , 1.095707  , 0.        ],\n",
      "       [0.        , 1.4783723 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 1.5830462 , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.        , 0.        ]], dtype=float32)>, <tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(100, 3), dtype=float32, numpy=\n",
      "array([[-3.56624997e-03,  1.48050213e+00,  1.20393729e+00],\n",
      "       [ 2.12380338e+00,  3.03042293e+00, -1.01011240e+00],\n",
      "       [ 4.46343303e-01,  2.29647398e+00, -6.65406883e-01],\n",
      "       [-1.25036228e+00,  1.25397515e+00, -5.48270524e-01],\n",
      "       [ 6.13885581e-01,  1.01336753e+00, -2.11520052e+00],\n",
      "       [ 1.44755423e+00,  2.81090289e-01,  1.30224299e+00],\n",
      "       [-1.32862079e+00, -8.79157484e-02, -4.73244071e-01],\n",
      "       [ 1.12259471e+00,  3.61768961e-01, -1.17964089e+00],\n",
      "       [ 1.74025154e+00, -1.24339521e+00, -6.75794184e-01],\n",
      "       [ 6.97475791e-01, -1.01987314e+00,  8.14581633e-01],\n",
      "       [-2.40011811e-01,  1.54686451e+00,  4.98074830e-01],\n",
      "       [ 1.19939551e-01,  7.12506354e-01,  1.88586414e+00],\n",
      "       [ 9.93054390e-01,  3.82969677e-01,  6.63735926e-01],\n",
      "       [-1.33913088e+00, -1.53470087e+00,  6.36534631e-01],\n",
      "       [-3.77411276e-01,  1.65970206e+00,  1.36926806e+00],\n",
      "       [ 4.51349944e-01,  1.17058432e+00,  9.36089694e-01],\n",
      "       [-2.95726731e-02,  1.43200481e+00, -2.66111165e-01],\n",
      "       [ 2.06738383e-01,  3.06893468e+00,  1.53922808e+00],\n",
      "       [-1.52363896e+00,  8.16120505e-01, -1.44744897e+00],\n",
      "       [-1.29526615e+00, -7.63714910e-01,  2.27625585e+00],\n",
      "       [-1.12435007e+00,  8.06392252e-01,  5.69558799e-01],\n",
      "       [ 2.72079408e-01,  6.14487648e-01, -1.11950934e+00],\n",
      "       [ 1.30126762e+00,  1.17008710e+00, -7.92369246e-01],\n",
      "       [ 5.59688330e-01,  1.48169637e+00,  1.17297101e+00],\n",
      "       [-3.82741839e-01,  1.16016650e+00,  4.33086693e-01],\n",
      "       [ 1.23301888e+00,  1.45381987e+00,  9.61718261e-01],\n",
      "       [ 1.10890567e+00, -9.15996265e-03, -5.68071485e-01],\n",
      "       [-1.30688608e+00,  8.37751091e-01,  6.37138069e-01],\n",
      "       [-4.71889168e-01,  8.73541057e-01, -2.87842993e-02],\n",
      "       [ 1.27503943e+00,  3.98348451e-01,  1.02621424e+00],\n",
      "       [-7.33893931e-01, -2.04356718e+00, -1.18515849e+00],\n",
      "       [ 5.19982517e-01,  1.17763197e+00, -2.69755304e-01],\n",
      "       [ 5.82620323e-01,  1.33924139e+00, -1.50726572e-01],\n",
      "       [-3.70777637e-01,  2.80030221e-02, -1.48159039e+00],\n",
      "       [-9.31257308e-01,  8.95997763e-01, -1.34780300e+00],\n",
      "       [ 6.69902742e-01,  6.47372127e-01, -1.29138744e+00],\n",
      "       [ 8.75941515e-01, -5.48107326e-01,  1.13601959e+00],\n",
      "       [-2.26103649e-01,  1.17787480e+00, -7.74420023e-01],\n",
      "       [-1.69850454e-01,  2.10369915e-01, -2.91352838e-01],\n",
      "       [ 8.24912548e-01, -6.31256282e-01,  9.48037088e-01],\n",
      "       [-3.99682641e-01, -8.52147877e-01,  5.62013447e-01],\n",
      "       [ 2.59244502e-01,  2.72049594e+00, -2.87722196e-05],\n",
      "       [-1.05114210e+00,  1.95229340e+00,  1.48888206e+00],\n",
      "       [ 9.49292660e-01,  1.32792866e+00, -2.20153145e-02],\n",
      "       [-7.23243177e-01,  1.98654258e+00,  4.78218645e-01],\n",
      "       [ 1.85368538e-01,  6.42461181e-01,  3.04436505e-01],\n",
      "       [-6.10185325e-01, -1.38986027e+00,  2.09000945e+00],\n",
      "       [ 3.37883905e-02,  1.46243262e+00,  2.32056409e-01],\n",
      "       [ 1.86001301e+00,  2.63755918e-01,  4.85580415e-01],\n",
      "       [ 1.87582761e-01, -2.39648819e-02,  6.25859916e-01],\n",
      "       [-1.57647586e+00, -4.29751664e-01, -8.59538186e-03],\n",
      "       [-7.59695411e-01,  9.19679999e-01,  1.73713937e-01],\n",
      "       [ 1.82969481e-01,  6.02902889e-01,  8.10459405e-02],\n",
      "       [-1.44554079e+00,  2.80287921e-01, -9.45107579e-01],\n",
      "       [-1.13704669e+00, -7.13170707e-01,  6.81686342e-01],\n",
      "       [ 1.45740747e+00,  1.91188419e+00, -8.88755202e-01],\n",
      "       [ 9.14705098e-01,  3.40961862e+00, -5.57765365e-01],\n",
      "       [-1.64553940e+00,  1.02571893e+00,  9.29194465e-02],\n",
      "       [ 9.46076989e-01,  4.58941698e+00, -6.82315290e-01],\n",
      "       [-3.27315450e-01, -4.14380401e-01, -8.12542975e-01],\n",
      "       [ 6.67854369e-01, -2.23089904e-01,  8.81088853e-01],\n",
      "       [-1.84288132e+00, -2.40963757e-01,  1.50801265e+00],\n",
      "       [ 2.24754706e-01, -1.34415066e+00,  1.05613279e+00],\n",
      "       [ 8.31403971e-01,  4.26894054e-02, -5.18698335e-01],\n",
      "       [-1.03155732e+00,  1.70627046e+00,  1.50357044e+00],\n",
      "       [-4.38112579e-02,  1.19186783e+00, -3.39672863e-01],\n",
      "       [ 3.82655054e-01,  2.43647099e-01,  2.92065382e-01],\n",
      "       [-6.66558683e-01,  3.74608010e-01, -2.08789968e+00],\n",
      "       [-8.04820836e-01,  6.72239208e+00,  1.60255134e-01],\n",
      "       [ 1.66524529e+00,  5.52785516e-01, -1.51174814e-01],\n",
      "       [-9.75446641e-01,  4.72089434e+00, -1.04592526e+00],\n",
      "       [ 7.52544403e-01,  4.83506650e-01,  5.75395487e-02],\n",
      "       [ 5.10048866e-01,  1.04068375e+00, -1.41499746e+00],\n",
      "       [ 5.53254426e-01,  2.85625666e-01, -4.53488499e-01],\n",
      "       [ 1.14231884e+00,  1.96620560e+00,  1.99874982e-01],\n",
      "       [-4.98397440e-01,  3.29934692e+00, -2.47235879e-01],\n",
      "       [ 4.01293457e-01, -1.10866606e+00,  7.07351387e-01],\n",
      "       [-2.25993380e-01, -2.39423537e+00, -5.53049855e-02],\n",
      "       [-1.60753739e+00, -1.61964071e+00,  1.68959022e+00],\n",
      "       [-1.64695251e+00, -1.91968545e-01,  1.11104453e+00],\n",
      "       [ 1.11389291e+00,  1.04404904e-01, -3.36462587e-01],\n",
      "       [-3.13199162e-01,  1.73879719e+00, -1.29814136e+00],\n",
      "       [-1.51440307e-01, -1.02764308e-01,  1.29495287e+00],\n",
      "       [-1.27347147e+00,  1.08319819e+00, -3.97582769e-01],\n",
      "       [ 1.58961260e+00,  1.50414467e+00,  6.97295070e-01],\n",
      "       [ 1.58429563e+00,  2.76121449e+00,  1.86019003e+00],\n",
      "       [ 9.49228108e-01, -9.41682160e-01,  1.56821871e+00],\n",
      "       [ 2.25076461e+00, -2.03768030e-01,  8.79892111e-01],\n",
      "       [-1.22338331e+00, -4.53880101e-01,  1.02462065e+00],\n",
      "       [ 9.94391203e-01,  6.09320045e-01,  2.04882169e+00],\n",
      "       [ 1.00911367e+00, -1.20639324e-01,  1.22203696e+00],\n",
      "       [-1.05354559e+00, -2.35642552e+00,  1.03231740e+00],\n",
      "       [-2.80092597e-01,  1.59832764e+00, -1.47130579e-01],\n",
      "       [-4.04976487e-01,  1.64579916e+00,  7.83583447e-02],\n",
      "       [ 1.09466171e+00,  1.26647949e+00,  1.87036777e+00],\n",
      "       [-1.07816255e+00,  9.20595050e-01,  3.63777608e-01],\n",
      "       [-1.34187317e+00,  1.66420758e+00,  1.15021908e+00],\n",
      "       [ 1.51166499e+00,  6.34588957e-01,  1.79187849e-01],\n",
      "       [ 3.73099029e-01,  1.30613410e+00,  4.93166208e-01],\n",
      "       [ 3.05581838e-01,  7.60823786e-01, -1.91157311e-01]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "a=Data_Train_Flat[0:100]\n",
    "b=vae_40MHZ.encoder(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f24034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
