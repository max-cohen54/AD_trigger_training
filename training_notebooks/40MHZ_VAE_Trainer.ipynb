{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 18:32:56.992197: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-09 18:32:57.208296: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import sklearn\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "from scipy.optimize import curve_fit\n",
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.math as tfmath\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN_VAE_Model(Model):\n",
    "    def __init__(self,input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "        super(DNN_VAE_Model,self).__init__()\n",
    "        self.e1=layers.Dense(h_dim_1, activation='relu')\n",
    "        self.e2=layers.Dense(h_dim_2, activation='relu')\n",
    "        self.mean=layers.Dense(latent_dim, activation='relu')\n",
    "        self.logvar=layers.Dense(latent_dim, activation='relu')\n",
    "\n",
    "        self.d1=layers.Dense(h_dim_2, activation='relu')\n",
    "        self.d2=layers.Dense(h_dim_1, activation='relu')\n",
    "        self.d3=layers.Dense(input_dim)\n",
    "        \n",
    "    def sample(self,m,logvar):\n",
    "        std=tfmath.exp(0.5*logvar)\n",
    "        eps=tf.random.normal(tf.shape(m))\n",
    "        x=m+eps*std\n",
    "        return x\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x= self.e1(inputs)\n",
    "        x= self.e2(x)\n",
    "        m=self.mean(x)\n",
    "        logvar=self.logvar(x)\n",
    "        self.m=m\n",
    "        self.lv=logvar\n",
    "\n",
    "        x=self.sample(m,logvar)\n",
    "\n",
    "        x=self.d1(x)\n",
    "        x=self.d2(x)\n",
    "        z=self.d3(x)\n",
    "        return z\n",
    "    \n",
    "    def loss_function(self,y_true,y_pred,beta=1):\n",
    "        \"\"\"masked mse\"\"\"\n",
    "        mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "        squared_difference = K.square(mask * (y_pred - y_true))\n",
    "        L1= K.mean(squared_difference)\n",
    "\n",
    "        mu=self.m\n",
    "        logv=self.lv\n",
    "\n",
    "        KL=-0.5*K.mean(1+logv-mu**2 - tfmath.exp(logv))\n",
    "\n",
    "        loss=L1+beta*KL\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(m,logvar):\n",
    "    std=tfmath.exp(0.5*logvar)\n",
    "    eps=tf.random.normal(tf.shape(m))\n",
    "\n",
    "    x=m+eps*std\n",
    "    return x\n",
    "    \n",
    "\n",
    "def create_VAE(input_dim, h_dim_1, h_dim_2, latent_dim):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(inputs)\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    m = layers.Dense(latent_dim, activation='relu')(x)\n",
    "    logvar = layers.Dense(latent_dim, activation='relu')(x)\n",
    "\n",
    "    x = sample(m,logvar)\n",
    "\n",
    "    x = layers.Dense(h_dim_2, activation='relu')(x)\n",
    "    x = layers.Dense(h_dim_1, activation='relu')(x)\n",
    "    outputs = layers.Dense(input_dim)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_true, y_pred):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    return K.mean(squared_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss_function(y_true,y_pred,beta=1):\n",
    "    \"\"\"masked mse\"\"\"\n",
    "    mask = K.cast(K.not_equal(y_true, 0), K.floatx())\n",
    "    squared_difference = K.square(mask * (y_pred - y_true))\n",
    "    L1= K.mean(squared_difference)\n",
    "\n",
    "    mu=mu_from_run\n",
    "    logv=logv_from_run\n",
    "\n",
    "    KL=-0.5*K.mean(1+logv-mu**2 - tfmath.exp(logv))\n",
    "\n",
    "    loss=L1+beta*KL\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=h5py.File('/eos/home-w/wsherman/AD_Work/n_tuples/40MHZ_data/background_for_training.h5','r')\n",
    "Dataset=np.array(f[\"Particles\"])\n",
    "\n",
    "truthtable=[]\n",
    "\n",
    "threshold=50\n",
    "for i, batch in enumerate(Dataset):\n",
    "  if np.sum(batch[:,0])>=threshold:\n",
    "    truthtable+=[1]\n",
    "  else:\n",
    "    truthtable+=[0]\n",
    "\n",
    "event_pt_br=[]\n",
    "Data_Test_full=Dataset[2000001:3600000,:,:]\n",
    "for j, br_1 in enumerate(Data_Test_full):\n",
    "  event_pt_br+=[np.sum(br_1[:,0])]\n",
    "\n",
    "for i, batch in enumerate(Dataset):\n",
    "  pt_sum=0\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    if particle[3]!=0:\n",
    "      pt_sum+=particle[0]\n",
    "  for j, particle in enumerate(Dataset[i,:,:]):\n",
    "    particle[0]=particle[0]/pt_sum\n",
    "\n",
    "Data_Train=Dataset[0:2000000,:,0:3]\n",
    "Data_Test=Dataset[2000001:3600000,:,0:3]\n",
    "Test_Truth=truthtable[2000001:3600000]\n",
    "Data_Validate=Dataset[3600001:4000000,:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Train_Flat=np.reshape(Data_Train,(-1,57))\n",
    "Data_Val_Flat=np.reshape(Data_Validate,(-1,57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dnn_vae__model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_100 (Dense)           multiple                  1856      \n",
      "                                                                 \n",
      " dense_101 (Dense)           multiple                  528       \n",
      "                                                                 \n",
      " dense_102 (Dense)           multiple                  136       \n",
      "                                                                 \n",
      " dense_103 (Dense)           multiple                  136       \n",
      "                                                                 \n",
      " dense_104 (Dense)           multiple                  144       \n",
      "                                                                 \n",
      " dense_105 (Dense)           multiple                  544       \n",
      "                                                                 \n",
      " dense_106 (Dense)           multiple                  1881      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,225\n",
      "Trainable params: 5,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = 57\n",
    "H_DIM_1 = 32\n",
    "H_DIM_2 = 16\n",
    "LATENT_DIM = 8\n",
    "DNN_VAE = DNN_VAE_Model(input_dim=INPUT_DIM, h_dim_1=H_DIM_1, h_dim_2=H_DIM_2, latent_dim=LATENT_DIM)\n",
    "DNN_VAE.build((1,57))\n",
    "DNN_VAE.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1954/1954 [==============================] - 13s 5ms/step - loss: 0.2165 - val_loss: 0.2154 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2156 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1954/1954 [==============================] - 11s 6ms/step - loss: 0.2156 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1952/1954 [============================>.] - ETA: 0s - loss: 0.2156\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "1951/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "1950/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 15/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 16/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 17/50\n",
      "1952/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 19/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 20/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 21/50\n",
      "1948/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-07\n",
      "Epoch 22/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 23/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 24/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 25/50\n",
      "1948/1954 [============================>.] - ETA: 0s - loss: 0.2155\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-08\n",
      "Epoch 26/50\n",
      "1954/1954 [==============================] - 10s 5ms/step - loss: 0.2155 - val_loss: 0.2153 - lr: 1.0000e-09\n"
     ]
    }
   ],
   "source": [
    "DNN_VAE.compile(optimizer='adam', loss=DNN_VAE.loss_function)\n",
    "\n",
    "STOP_PATIENCE = 8\n",
    "LR_PATIENCE = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=STOP_PATIENCE, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=LR_PATIENCE, verbose=1)\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "history = DNN_VAE.fit(x=Data_Train_Flat, y=Data_Train_Flat, validation_data=(Data_Val_Flat,Data_Val_Flat), epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_VAE.save('/eos/home-w/wsherman/AD_Work/ML_git_repo/AD_trigger_training/trained_models/40MHZ_norm_DNN_VAE.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
